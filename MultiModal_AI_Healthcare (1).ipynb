{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ArXiv retrieval"
      ],
      "metadata": {
        "id": "STMCCwoArr63"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTMmJUWDaM1_",
        "outputId": "700822b7-7780-4d62-ceaf-f431bae58c13"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2024.12.14)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=390ed0ed0e3a6879e6fcbfea2ee54e687ca26628d52ab8a3b22872784a05a10f\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzsJaVOXzq5Z",
        "outputId": "e2f11d3e-e6f8-4385-d7cb-a8062fc886f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting arxiv\n",
            "  Downloading arxiv-2.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Collecting feedparser~=6.0.10 (from arxiv)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.10->arxiv)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2024.12.14)\n",
            "Downloading arxiv-2.1.3-py3-none-any.whl (11 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6047 sha256=23db6e8e3a1bfb63f7304cc75d5e6982d7834950587114c1dd75ee0d732b352c\n",
            "  Stored in directory: /root/.cache/pip/wheels/f0/69/93/a47e9d621be168e9e33c7ce60524393c0b92ae83cf6c6e89c5\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, arxiv\n",
            "Successfully installed arxiv-2.1.3 feedparser-6.0.11 sgmllib3k-1.0.0\n",
            "Fetching results for Multimodal papers in Healthcare from arXiv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-16807dd95ebf>:21: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to multimodal_healthcare_complete_results.csv\n",
            "  Database           DOI                                              Title  \\\n",
            "0    arXiv  2412.14660v1  Unveiling Uncertainty: A Deep Dive into Calibr...   \n",
            "1    arXiv  2405.08603v2  A Comprehensive Survey of Large Language Model...   \n",
            "2    arXiv  2412.00103v1  MLLM-Search: A Zero-Shot Approach to Finding P...   \n",
            "3    arXiv  2410.01812v5  From Text to Multimodality: Exploring the Evol...   \n",
            "4    arXiv  2406.19280v4  HuatuoGPT-Vision, Towards Injecting Medical Vi...   \n",
            "\n",
            "                                            Abstract  Year  \\\n",
            "0  Multimodal large language models (MLLMs) combi...  2024   \n",
            "1  Since the release of ChatGPT and GPT-4, large ...  2024   \n",
            "2  Robotic search of people in human-centered env...  2024   \n",
            "3  Large Language Models (LLMs) have rapidly evol...  2024   \n",
            "4  The rapid development of multimodal large lang...  2024   \n",
            "\n",
            "                                             Authors  \\\n",
            "0  Zijun Chen, Wenbo Hu, Guande He, Zhijie Deng, ...   \n",
            "1  Hanguang Xiao, Feizhong Zhou, Xingyue Liu, Tia...   \n",
            "2  Angus Fung, Aaron Hao Tan, Haitong Wang, Beno ...   \n",
            "3  Qian Niu, Keyu Chen, Ming Li, Pohsun Feng, Ziq...   \n",
            "4  Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe ...   \n",
            "\n",
            "                                            Keywords  \n",
            "0  calibration, mllms, multimodal, uncertainty, v...  \n",
            "1              language, llms, mllms, models, survey  \n",
            "2        approach, environment, mllm, person, search  \n",
            "3             medical, mllms, models, practice, text  \n",
            "4           data, medical, mllms, multimodal, vision  \n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv pandas scikit-learn\n",
        "\n",
        "import arxiv\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "query = '(\"Multimodal Large Language Model*\" OR MLLM* OR MM-LLM* OR \"Information Fusion\" OR \"Multimodal Learn*\" OR \"Joint Learn*\" OR \"Cross Learn*\") AND (Healthcare OR Medicine OR Health)'\n",
        "\n",
        "def fetch_all_arxiv_results(query, total_results=800, results_per_page=100):\n",
        "    all_results = []\n",
        "    client = arxiv.Client(page_size=results_per_page)\n",
        "\n",
        "    for start in range(0, total_results, results_per_page):\n",
        "        search = arxiv.Search(\n",
        "            query=query,\n",
        "            max_results=results_per_page,\n",
        "            sort_by=arxiv.SortCriterion.Relevance\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            for result in search.results():\n",
        "                all_results.append({\n",
        "                    \"Database\": \"arXiv\",\n",
        "                    \"DOI\": result.entry_id.split('/')[-1],\n",
        "                    \"Title\": result.title,\n",
        "                    \"Abstract\": result.summary,\n",
        "                    \"Year\": result.updated.year,\n",
        "                    \"Authors\": \", \".join([author.name for author in result.authors])\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching results from start={start}: {e}\")\n",
        "            break\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def extract_keywords(texts, num_keywords=5):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_features=num_keywords)\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    keywords = vectorizer.get_feature_names_out()\n",
        "    return keywords\n",
        "\n",
        "\n",
        "print(\"Fetching results for Multimodal papers in Healthcare from arXiv...\")\n",
        "results = fetch_all_arxiv_results(query, total_results=800, results_per_page=100)\n",
        "\n",
        "if not results:\n",
        "    print(\"No results found. Please check the query syntax or try different keywords.\")\n",
        "else:\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df[\"Text\"] = df[\"Title\"] + \" \" + df[\"Abstract\"]\n",
        "    df[\"Keywords\"] = df[\"Text\"].apply(lambda x: \", \".join(extract_keywords([x])))\n",
        "    df.drop(columns=[\"Text\"], inplace=True)\n",
        "\n",
        "    csv_filename = \"multimodal_healthcare_complete_results.csv\"\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Results saved to {csv_filename}\")\n",
        "\n",
        "    print(df.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRdWx9Lz0uiO",
        "outputId": "40a08608-67d2-47c0-e948-8609de4d7b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: arxiv in /usr/local/lib/python3.10/dist-packages (2.1.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.6.0)\n",
            "Requirement already satisfied: feedparser~=6.0.10 in /usr/local/lib/python3.10/dist-packages (from arxiv) (6.0.11)\n",
            "Requirement already satisfied: requests~=2.32.0 in /usr/local/lib/python3.10/dist-packages (from arxiv) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.10/dist-packages (from feedparser~=6.0.10->arxiv) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests~=2.32.0->arxiv) (2024.12.14)\n",
            "Fetching all results for Multimodal papers in Healthcare from arXiv...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-18-a0cdacb1bc0f>:26: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  page_results = list(search.results())\n"
          ]
        }
      ],
      "source": [
        "!pip install arxiv pandas scikit-learn\n",
        "\n",
        "import arxiv\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "query = '(\"Multimodal\" OR \"Multi-modal\" OR \"Information Fusion\" OR \"Multimodal Learning\") AND (Healthcare OR Medicine OR Health)'\n",
        "\n",
        "def fetch_all_arxiv_results(query, results_per_page=100):\n",
        "    all_results = []\n",
        "    client = arxiv.Client(page_size=results_per_page)\n",
        "    start = 0\n",
        "\n",
        "    while True:\n",
        "        search = arxiv.Search(\n",
        "            query=query,\n",
        "            max_results=results_per_page,\n",
        "            sort_by=arxiv.SortCriterion.Relevance\n",
        "        )\n",
        "\n",
        "        try:\n",
        "            page_results = list(search.results())\n",
        "            if not page_results:\n",
        "                break\n",
        "            for result in page_results:\n",
        "                all_results.append({\n",
        "                    \"Database\": \"arXiv\",\n",
        "                    \"DOI\": result.entry_id.split('/')[-1],\n",
        "                    \"Title\": result.title,\n",
        "                    \"Abstract\": result.summary,\n",
        "                    \"Year\": result.updated.year,\n",
        "                    \"Authors\": \", \".join([author.name for author in result.authors])\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching results: {e}\")\n",
        "            break\n",
        "\n",
        "        start += results_per_page\n",
        "\n",
        "    return all_results\n",
        "\n",
        "def extract_keywords(texts, num_keywords=5):\n",
        "    vectorizer = TfidfVectorizer(stop_words='english', max_features=num_keywords)\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    keywords = vectorizer.get_feature_names_out()\n",
        "    return keywords\n",
        "\n",
        "print(\"Fetching all results for Multimodal papers in Healthcare from arXiv...\")\n",
        "results = fetch_all_arxiv_results(query)\n",
        "\n",
        "if results:\n",
        "    df = pd.DataFrame(results)\n",
        "    df[\"Text\"] = df[\"Title\"] + \" \" + df[\"Abstract\"]\n",
        "    df[\"Keywords\"] = df[\"Text\"].apply(lambda x: \", \".join(extract_keywords([x])))\n",
        "    df.drop(columns=[\"Text\"], inplace=True)\n",
        "\n",
        "    csv_filename = \"arxiv_all_multimodal_healthcare_results_with_keywords.csv\"\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"All papers saved to {csv_filename}\")\n",
        "    print(df.head())\n",
        "\n",
        "else:\n",
        "    print(\"No results found for the given query.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PubMed retrieval"
      ],
      "metadata": {
        "id": "XtKdtyFhrz-t"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDKcGhqI4yfm"
      },
      "outputs": [],
      "source": [
        "!pip install metapub pandas tqdm\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from metapub import PubMedFetcher\n",
        "from tqdm import tqdm\n",
        "\n",
        "def pubmed_search(search, n_articles=1000):\n",
        "    fetch = PubMedFetcher()\n",
        "\n",
        "    pmids = fetch.pmids_for_query(search, retmax=n_articles)\n",
        "\n",
        "    # Initialize dictionaries to store data\n",
        "    articles = {}\n",
        "    titles = {}\n",
        "    abstracts = {}\n",
        "    authors = {}\n",
        "    years = {}\n",
        "    citations = {}\n",
        "    dois = {}\n",
        "\n",
        "    # Loop through PMIDs with a progress bar\n",
        "    for pmid in tqdm(pmids, desc=\"Fetching PubMed articles\"):\n",
        "        if pmid is not None:\n",
        "            article = fetch.article_by_pmid(pmid)\n",
        "            articles[pmid] = article\n",
        "            titles[pmid] = article.title\n",
        "            abstracts[pmid] = article.abstract\n",
        "            authors[pmid] = article.authors\n",
        "            years[pmid] = article.year\n",
        "            dois[pmid] = article.doi\n",
        "        else:\n",
        "            print(f\"Couldn't retrieve pmid {pmid}\")\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'doi': dois.values(),\n",
        "        'title': titles.values(),\n",
        "        'abstract': abstracts.values(),\n",
        "        'year': years.values(),\n",
        "        'authors': authors.values(),\n",
        "    })\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY = \"\"\"\n",
        "(\"Multimodal Large Language Model*\" OR MLLM* OR MM-LLM* OR \"Information Fusion\" OR \"Multimodal Learn*\" OR \"Joint Learn*\" OR \"Cross Learn*\") AND (Healthcare OR Medicine OR Health)\n",
        "\"\"\"\n",
        "\n",
        "pubmed = pubmed_search(QUERY)"
      ],
      "metadata": {
        "id": "vt3JPx4FsKKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scopus retrieval"
      ],
      "metadata": {
        "id": "NMagWSnuvqSk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybliometrics pandas\n",
        "\n",
        "import pybliometrics\n",
        "from pybliometrics.scopus import ScopusSearch\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def scopus_search(query, verbose=True, subscriber=False):\n",
        "  pybliometrics.scopus.init(keys=\"9bfd3532db84db8ee142557d9d1b654f\") # non-subscriber key\n",
        "\n",
        "  r = ScopusSearch(query, verbose=verbose, subscriber=subscriber) # does not allow retrievals over 5000 papers\n",
        "  df = pd.DataFrame(r.results)\n",
        "  df = df[[\"doi\", \"title\", \"description\", \"coverDisplayDate\", \"author_names\", \"authkeywords\"]]\n",
        "  df = df.rename(columns={\"doi\": \"Doi\", \"title\": \"Title\", \"description\": \"Abstract\", \"coverDisplayDate\": \"Year\", \"author_names\": \"Authors\", \"authkeywords\": \"Keywords\"})\n",
        "  df[\"Year\"] = df[\"Year\"].str.split(\" \").str[-1]\n",
        "  df[\"Database\"] = \"Scopus\"\n",
        "\n",
        "  return df"
      ],
      "metadata": {
        "id": "TCwgmUSevtAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QUERY1 = \"\"\"\n",
        "(\"Multimodal Large Language Model*\" OR MLLM* OR MM-LLM* OR \"Information Fusion\" OR \"Multimodal Learn*\" OR \"Joint Learn*\" OR \"Cross Learn*\") AND (Healthcare OR Medicine OR Health) AND PUBYEAR = 2025\n",
        "\"\"\" # this search just contains papers from 2025, since 2024 already has over 16k papers and exceeds the limits to non-subscribers\n",
        "\n",
        "scopus = scopus_search(QUERY1)"
      ],
      "metadata": {
        "id": "0oYhAg9vVpem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IEEEXplore retrieval"
      ],
      "metadata": {
        "id": "fiLFwkoLWnOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ieeexplore-api\n",
        "\n",
        "import pandas as pd\n",
        "from ieeexplore_api import IEEExploreAPI\n",
        "\n",
        "def ieee_search(query):\n",
        "    try:\n",
        "        api = IEEExploreAPI(\"a55guwmbva59yxf6kfzwwuks\") # still waiting for approval\n",
        "        results = api.search(query)\n",
        "\n",
        "        if not results:\n",
        "            print(\"No results found for the query.\")\n",
        "            return None\n",
        "\n",
        "        data = []\n",
        "        for record in results['records']:\n",
        "            authors = ', '.join(record.get('authors', []))\n",
        "            keywords = ', '.join(record.get('keywords', []))\n",
        "            data.append({\n",
        "                'Database': 'IEEE Xplore',\n",
        "                'DOI': record.get('doi'),\n",
        "                'Title': record.get('title'),\n",
        "                'Abstract': record.get('abstract'),\n",
        "                'Year': record.get('publication_year'),\n",
        "                'Authors': authors,\n",
        "                'Keywords': keywords,\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "    except Exception as e:\n",
        "        print(f\"Error during IEEE Xplore search: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "PONFQGrRWusp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ieee = ieee_search(QUERY)"
      ],
      "metadata": {
        "id": "M0kZZ9TrX8hg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WOS retrieval"
      ],
      "metadata": {
        "id": "GHPnK5l7aNvX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wosplus\n",
        "\n",
        "from wosplus import WosClient\n",
        "import pandas as pd\n",
        "\n",
        "def wos_search(query):\n",
        "    try:\n",
        "        wos_client = WosClient(key=\"API_KEY\")  # still waiting for approval\n",
        "        query_results = wos_client.query(query)\n",
        "\n",
        "        if not query_results:\n",
        "            print(\"No results found for the query.\")\n",
        "            return None\n",
        "\n",
        "        data = []\n",
        "        for record in query_results:\n",
        "            data.append({\n",
        "                'Database': 'Web of Science',\n",
        "                'DOI': record.get('DI'),\n",
        "                'Title': record.get('TI'),\n",
        "                'Abstract': record.get('AB'),\n",
        "                'Year': record.get('PY'),\n",
        "                'Authors': record.get('AU')\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during Web of Science search: {e}\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "FELEWDQ1aJkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wos = wos_search(QUERY)"
      ],
      "metadata": {
        "id": "-U7_UsrWa47v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv pandas PyPDF2 scikit-learn tqdm\n",
        "\n",
        "# Import required libraries\n",
        "import arxiv\n",
        "import pandas as pd\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from PyPDF2 import PdfReader\n",
        "import os\n",
        "\n",
        "# Define search query\n",
        "query = (\n",
        "    '(\"Multimodal Large Language Model*\" OR MLLM* OR MM-LLM* OR \"Information Fusion\" '\n",
        "    'OR \"Multimodal Learn*\" OR \"Cross-modal\" OR \"Cross-modal Learning\" OR \"Cross-modal Fusion\") '\n",
        "    'AND (Healthcare OR Medicine OR Health)'\n",
        ")\n",
        "\n",
        "# Directory to store downloaded PDFs\n",
        "pdf_dir = \"arxiv_pdfs\"\n",
        "os.makedirs(pdf_dir, exist_ok=True)\n",
        "\n",
        "# Function to fetch results from ArXiv\n",
        "def fetch_all_arxiv_results(query, results_per_page=100):\n",
        "    all_results = []\n",
        "    client = arxiv.Client(page_size=results_per_page)\n",
        "    start = 0\n",
        "\n",
        "    while True:\n",
        "        search = arxiv.Search(\n",
        "            query=query,\n",
        "            max_results=results_per_page,\n",
        "            sort_by=arxiv.SortCriterion.Relevance\n",
        "        )\n",
        "        try:\n",
        "            page_results = list(search.results())\n",
        "            if not page_results:\n",
        "                break\n",
        "            for result in page_results:\n",
        "                all_results.append({\n",
        "                    \"Title\": result.title,\n",
        "                    \"Abstract\": result.summary,\n",
        "                    \"Authors\": \", \".join([author.name for author in result.authors]),\n",
        "                    \"Year\": result.updated.year,\n",
        "                    \"PDF_URL\": result.pdf_url\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching results: {e}\")\n",
        "            break\n",
        "    return all_results\n",
        "\n",
        "# Function to download PDFs\n",
        "def download_pdf(pdf_url, filename):\n",
        "    try:\n",
        "        response = requests.get(pdf_url)\n",
        "        with open(filename, \"wb\") as f:\n",
        "            f.write(response.content)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download {pdf_url}: {e}\")\n",
        "        return False\n",
        "\n",
        "def extract_text_from_pdf(filepath):\n",
        "    try:\n",
        "        reader = PdfReader(filepath)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {filepath}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "def extract_keywords(texts, num_keywords=5):\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=num_keywords)\n",
        "    tfidf_matrix = vectorizer.fit_transform(texts)\n",
        "    return vectorizer.get_feature_names_out()\n",
        "\n",
        "# Fetch all results\n",
        "print(\"Fetching all results from ArXiv...\")\n",
        "results = fetch_all_arxiv_results(query)\n",
        "\n",
        "if results:\n",
        "    df = pd.DataFrame(results)\n",
        "\n",
        "    # Download PDFs and extract content\n",
        "    print(\"Downloading and processing PDFs...\")\n",
        "    full_texts = []\n",
        "    for index, row in tqdm(df.iterrows(), total=len(df)):\n",
        "        pdf_filename = os.path.join(pdf_dir, f\"{index}.pdf\")\n",
        "        if download_pdf(row[\"PDF_URL\"], pdf_filename):\n",
        "            full_text = extract_text_from_pdf(pdf_filename)\n",
        "            full_texts.append(full_text)\n",
        "        else:\n",
        "            full_texts.append(\"\")\n",
        "\n",
        "    # Add full texts to DataFrame\n",
        "    df[\"Full_Text\"] = full_texts\n",
        "\n",
        "    # Extract keywords from full text\n",
        "    print(\"Extracting keywords from full text...\")\n",
        "    df[\"Keywords\"] = df[\"Full_Text\"].apply(lambda x: \", \".join(extract_keywords([x])))\n",
        "\n",
        "    # Save results to CSV\n",
        "    csv_filename = \"arxiv_full_text_results.csv\"\n",
        "    df.to_csv(csv_filename, index=False)\n",
        "    print(f\"Results saved to {csv_filename}\")\n",
        "else:\n",
        "    print(\"No results found.\")"
      ],
      "metadata": {
        "id": "NhPIifC_cKOD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}