{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"Answer","type":"string"},{"name":"Question","type":"string"},{"name":"Tool","type":"string"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":[{"index":0,"Answer":{"source":"Papers\/From Text to Multimodality_ Exploring the Evolution and Impact of Large Language Models in Medical Practice.pdf (page 3)","content":"B. Multimodality in Medicine: Embracing the Rich Variety of texts. Med-Flamingo [15] incorporates a vision trans-\nData former for medical image understanding. Similar ap-\nMedicine is inherently multimodal, as it involves many proaches can be seen in models like SkinGPT-4 [16],\ntypes of information beyond just written text. For example, which combines a vision transformer with a LLM for\nwhen a patient comes in with a possible lung infection, their dermatologicaldiagnosis,andMedVersa[34],whichuses\ncase might include several kinds of data: written information a LLM as a learnable orchestrator to process both visual\nlike their medical history and symptoms noted by doctors, and linguistic information to interpret medical images.\nimagesfromchestX-rays,soundrecordingsoftheirbreathing, • Tools Assistance: These methods uses external tools for\nand even genetic information to assess their personal risk. multimodalities. A knowledge graph can link concepts\nCombining these different types of information is important across modalities and provide additional context for the\nfor getting a complete picture of a patient’s health and more LLM. The study by Gao et al. [20] uses the Unified\naccurate and personalized medical care [51]. This is where Medical Language System (UMLS) knowledge to en-\nmultimodalmodelsshine,becausetheyaredesignedtoprocess hance diagnosis generation. A similar approach is used\nand integrate various types of data. in BioLORD-2023 [21], which integrates LLMs with\nWe have seen a surge in developing MLLMs capable of knowledge graphs to improve performance in semantic\nprocessingandintegratingdiversemedicaldatatypes[52].The textualsimilarity, biomedicalconceptrepresentation,and\nstudy by Tian et al. [53] exemplifies this trend, showcasing named entity linking.\na Med-MLLM model that handles both visual and textual • Data-Driven Methods: These methods rely on large-\ndataforimprovedclinicaldecision-making,particularlyinrare scale multimodal datasets to train LLMs directly on"},"Question":"What is the importance of combining different types of information in medicine?","Tool":"similarity_search"},{"index":1,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 0)","content":"HuatuoGPT-Vision, Towards Injecting Medical Visual\nKnowledge into Multimodal LLMs at Scale\nJunyingChen1,2,ChiGui2,RuyiOuyang2,AnningzheGao1,2,ShunianChen1,2\nGuimingHardyChen1,2,XidongWang1,2,RuifeiZhang1,2,ZhenyangCai1,2,KeJi1,2\nGuangjunYu1,2,3,XiangWan1,2,3,BenyouWang1,2∗\n1ShenzhenResearchInstituteofBigData\n2TheChineseUniversityofHongKong,Shenzhen\n3NationalHealthDataInstitute,Shenzhen\nhttps:\/\/github.com\/FreedomIntelligence\/HuatuoGPT-Vision\nhttps:\/\/huggingface.co\/datasets\/FreedomIntelligence\/PubMedVision\nAbstract\nTherapiddevelopmentofmultimodallargelanguagemodels(MLLMs),suchas\nGPT-4V, has led tosignificant advancements. However, these modelsstill face\nchallengesinmedicalmultimodalcapabilitiesduetolimitationsinthequantity\nandqualityofmedicalvision-textdata,stemmingfromdataprivacyconcernsand\nhighannotationcosts. WhilepioneeringapproachesutilizePubMed’slarge-scale,\nde-identifiedmedicalimage-textpairstoaddresstheselimitations,theystillfall\nshort due to inherent data noise. To tackle this, we refined medical image-text\npairsfromPubMedandemployedMLLMs(GPT-4V)inan’unblinded’capacity\ntodenoiseandreformatthedata,resultinginthecreationofthePubMedVision\ndatasetwith1.3millionmedicalVQAsamples. Ourvalidationdemonstratesthat:\n(1)PubMedVisioncansignificantlyenhancethemedicalmultimodalcapabilities\nof current MLLMs, showing significant improvement in benchmarks including\ntheMMMUHealth&Medicinetrack;(2)manualchecksbymedicalexpertsand\nempiricalresultsvalidatethesuperiordataqualityofourdatasetcomparedtoother\ndataconstructionmethods. UsingPubMedVision,wetraina34BmedicalMLLM\nHuatuoGPT-Vision,whichshowssuperiorperformanceinmedicalmultimodal\nscenariosamongopen-sourceMLLMs.\n1 Introduction\nMultimodal Large Language Models (MLLMs), such as GPT4-V, show limited performance in\nmedicalapplications,particularlyinlackingvisualknowledgespecifictothemedicaldomain[1,2].\nAlthoughtherearesomesmall-scale,high-qualitydatasetscontainingmedicalvisualknowledge[3–"},"Question":"What are the limitations in medical multimodal capabilities due to the quantity and quality of medical vision-text data?","Tool":"similarity_search"},{"index":2,"Answer":{"source":"Papers\/From Text to Multimodality_ Exploring the Evolution and Impact of Large Language Models in Medical Practice.pdf (page 8)","content":"datasetsandmetricstonon-clinicaltasks,restrictingthepoten- [12] C. Zakka, R. Shad, A. Chaurasia, A. R. Dalal, J. L. Kim,\ntial to capture the full range of capabilities and biases [113]. M. Moor, R. Fong, C. Phillips, K. Alexander, E. Ashley\net al., “Almanac—retrieval-augmented language models for clinical\nTo ensure the safe and effective of clinical MLLMs, future\nmedicine,” NEJMAI,vol.1,no.2,p.AIoa2300068, 2024.\neffort should spend on developing additional standardized [13] Zekai Chen, Mariann Micsinai Balan, and Kevin Brown, “Language\nbenchmarks with closer clinical relevance. Models are Few-shot Learners for Prognostic Prediction,” arXiv.org,\n2023.\n[14] JiaweiChen,DingkangYang,YueJiang,YuxuanLei,andLihuaZhang,\nE. Ethics and Compliance “Miss: A Generative Pretraining and Finetuning Approach for Med-\nVQA,”arXiv.org,2024.\nThe ethical implications of MLLMs in healthcare cannot [15] Michael Moor,QianHuang,ShirleyWu,Michihiro Yasunaga, C.Za-\nbe overstated. Its training data contain sensitive patient in- kka,YashodharaDalmia,E.Reis,P.Rajpurkar,andJ.Leskovec,“Med-\nFlamingo:aMultimodalMedicalFew-shotLearner,”ML4H@NeurIPS,\nformation, raising concerns about privacy and data security\n2023.\n[29].Biasedtrainingdatacanleadtodiscriminatoryoutcomes, [16] J. Zhou, X. He, L. Sun, J. Xu, X. Chen, Y. Chu, L. Zhou, X. Liao,\npotentially exacerbating existing health disparities. Therefore, B. Zhang, S. Afvari, and X. Gao, “Pre-trained multimodal large\nlanguagemodelenhancesdermatologicaldiagnosisusingSkinGPT-4,”\nclear regulatory frameworks and guidelines are necessary to\nNatureCommunications, vol.15,no.1,jul52024.\ngovern the development, deployment, and use of MLLMs in [17] D. Dao, J. Y. C. Teo, W. Wang, and H. D. Nguyen, “Llm-powered\nclinicalsettings[35].Addressingtheseethicalandcompliance multimodal ai conversations for diabetes prevention,” in Proceedings\nofthe1stACMWorkshoponAI-PoweredQ&ASystemsforMultimedia,\nchallenges will be beneficial to establish trust and ensure the\n2024,pp.1–6."},"Question":"What are the potential consequences of biased training data in machine learning language models?","Tool":"similarity_search"},{"index":3,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 3)","content":"question-answering(QA),theystillrequiredtask-specificfine-tuningforeachdownstreamtask.Todevelopageneral\nlanguagemodelcapableofhandlingdiversetaskswithouttask-specificfine-tuning,Radfordetal.[47]curatedadataset\nofover8milliondocuments,totaling40GBoftextdata,encompassingexamplesfrommultipledomainsandtasks,and\nusedittotrainGPT-2.GPT-2setstate-of-the-artbenchmarkson7outof8languagemodelingtaskswithoutrequiring\ntask-specificfine-tuning.\nTo further enhance the generalization capabilities of language models, Brown et al. [4] scaled the model to 175\nbillion parameters and significantly expanded the training dataset. This resulted in GPT-3, which demonstrated a\nqualitative leap in performance, showcasing remarkable few-shot capabilities without requiring fine-tuning. GPT-3\ncouldhandleunfamiliartasksbasedsolelyonprovidedexamples,oftenachievingperformanceonparwithfine-tuned\nstate-of-the-artmodels.Asaresult,GPT-3iswidelyregardedasthebeginningofLLMs[35].TheproposalofGPT-3\nfurtherrevolutionizedNLPbyshiftingtheparadigmfromunsupervisedpre-training&fine-tuningtounsupervisedpre-\ntraining&prompt[39].Suchmodelscanhandlemosttaskseffectivelyusinguserpromptsandcontextualexamples.For\nexample,Flan-PaLM[13]achievedstate-of-the-artperformanceonMedQA,MedMCQA,PubMedQA,andMMLU\nclinicalbenchmarksbyemployingadvancedpromptingstrategies.However,thesemodelsarehighlysensitivetouser-\nprovidedprompts,whichdirectlyinfluencethequalityoftheirresponses.Thissensitivityhasspurredresearchersto\nexplorepromptingstrategiesindepth[48],initiatingashiftfromobjectiveengineeringtopromptengineering.\n2.4. Text-onlytoMultimodal\nInspiredbyGPT-3,researchershaveintensifiedeffortsindevelopingLLMs,resultinginprominentworkssuchas\nGLM-130B[49],PaLM[2,3],andLLaMA[6,7,8].However,theseLLMsremaintext-focused,anddespiteprogress\ninmultimodalresearch,theyoftenrequirefine-tuningfornewtasks[50,51]orlacktextgenerationcapabilities[52,53],\nwhichrestrictstheirapplicationscope.Inspiredbyfew-shotlearnerslikeGPT-3,Alayracetal.[54]curatedalarge-"},"Question":"Can a general language model handle diverse tasks without task-specific fine-tuning?","Tool":"similarity_search"},{"index":4,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 1)","content":"Concurrently, LLaVA-Med [7] uses a “blind” Large Language Model (LLM) to generate Visual\nQuestionAnswering(VQA)fromthecontextualtextofPubMedimages,achievingnotableresults.\nHowever,thisapproachmightoverlookvisualinformationinherentinthemedicalimagesthemselves\nasLLMscannotperceiveimagesasinput,probablyleadingtothegenerationofmisinterpretedor\nirrelevantanswers. Moreover,LLaVA-Medislimitedto56KmedicalVQAentries. Thus,creatinga\nhigher-qualityandlarger-scalevision-languagealignmentdatasetformedicineisessential.\nToclosethisgap,wemeticulouslyselecthigh-qualitymedicalimage-textpairfromPubMed,employ-\ningaproposedrefinedpipeline. Utilizing914,960refinedmedicalimagesandtheircorresponding\ntext,weapplyGPT-4Vasthe“unblinded”reformatter,contrastingthe“blinded”reformattingused\ninpreviousworks[7,8,6],todenoisethePubMeddata. Ourmethodgeneratesmorealignedmed-\nical VQA data for medical multimodal alignment. Consequently, we constructed a high-quality\nmultimodalmedicaldatasetwith1.3millionsamplesandnameitasPubMedVision.\nOur experiments validated PubMedVision in two key aspects: (1) It significantly enhances the\nmedicalmultimodalcapabilitiesofMLLMs,showingnotableimprovementinbenchmarkssuchas\nMMMUHealth&Medicine. LLaVA-v1.5-LLaMA-3-8Bachievesthestrongestperformanceamong\nopen-sourceMLLMswithPubMedVision; (2)Manualchecksbymedicalexpertsandempirical\nresultsconfirmedthesuperiordataqualityofPubMedVisioncomparedtocurrentdataconstruction\nmethods.\nThecontributionsofthispaperaresummarizedasfollows:\n1. Unblinded Data Reformatting for Medical Multimodality. We propose leveraging\n“unblinded” MLLMs to reformat PubMed image-text pairs to construct a better-aligned\nmedical VQA dataset. Expert reviews and empirical tests show that this method yields\nhigher-qualitydata,improvingMLLMtraining.\n2. PubMedVision: ALarge-scale,High-qualityMedicalMultimodalDataset. Withthe\nMLLM-powered reformatted method, we bulid PubMedVision, containing 1.3 million\nmedicalVQAentriesforvisualalignment. ExperimentsdemonstratethatPubMedVision"},"Question":"What is the main contribution of this paper?","Tool":"similarity_search"},{"index":5,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 18)","content":"G ScoringGuidelines\nDatasetScoringGuidelines\nPlease rate the image based on the following criteria (1 to 5):\n1. Accuracy: The rating should be based on whether the description\naccurately reflects the medical features and information in the image. A\nscore of 5 indicates complete accuracy, while a score of 1 indicates complete\ninaccuracy.\n2. Relevance: The rating should be based on the degree of tight association\nbetween the described content and the key medical features and information\nin the image, and whether it successfully avoids the depiction of irrelevant\ndetails in the image. A score of 5 indicates high relevance, while a score\nof 1 indicates complete irrelevance.\n3. Completeness: The rating should be based on whether the description\nincludes all the key medical features and information in the image. A\nscore of 5 indicates complete completeness, while a score of 1 indicates\nsignificant omissions.\n4. Practicality: The rating should be based on the extent to which\nthe description helps with medical decision-making, diagnosis, and\ntreatment planning. A score of 5 indicates high practicality with\nin-depth descriptions, while a score of 1 indicates that the description\nis superficial and not practical.\nFigure20: DatasetScoringGuidelines.\nH Limiations\nThePubMedVisiondatasethasseverallimitationsthatshouldbeconsidered:\n• HallucinationofMLLMs: TheconstructionofthePubMedVisiondatasetutilizesMLLM\nmodels(GPT-4V),whichasgenerativemodels,canproducehallucinationsorinaccuracies.\nThismightleadtoerrorsinthedataset. Futurestudiesmaybenefitfromimprovedvalidation\nprocessestomitigatethisissue.\n• LimitedScenarioDiversity: TheInstruction-TuningVQAofPubMedVisionaregenerated\nbasedon10predefinedscenarios. Thislimitedscopemayhaveconstrainedthediversityof\nthedataset. Expandingtherangeofscenariosinfutureworkcouldenhancethedataset’s\ncomprehensivenessandapplicabilitytoawiderarrayofmedicalsituations.\n• Data Selection: The rigorous image selection strategy during data preparation ensured"},"Question":"What is the primary consideration for errors in the dataset?","Tool":"similarity_search"},{"index":6,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 22)","content":"A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine\nexample, in scenarios demanding high privacy, complete device-side inference can be utilized to ensure full patient\nprivacyprotection.Inmorecomplexscenarios,thisarchitecturesupportsdynamicadjustmentsofmodelsplittingand\ntaskallocation.Forinstance,insurgicalassistance,theMECarchitecturecandeploylightweightmodelcomponents\nonedgedevicestoenablereal-timeresponses,whileoffloadingcomputationallyintensivetaskstonearbyedgeservers.\nThis setup supports distributed collaborative inference through 6G communication, reducing the burden on edge\ndevices.\n7.2. MedicalAgents\nClinical diagnosis is a multidisciplinary process requiring collaboration among experts from fields such as\npathology, radiology, and surgery. LLM- and MLLM-based agents can simulate departmental experts, facilitating\nteamcollaborationtodelivercomprehensiveandaccuratediagnosticsupport[15].\nFor example, when handling a complex medical case, a general practitioner agent can decompose the task into\nclearsub-tasksbasedonthepatient’sspecificconditionsandrequirements,dynamicallyassigningthemtothemost\nsuitable expert agents. Each expert agent provides analysis specific to their assigned sub-tasks. A radiology agent\nanalyzes imaging studies to generate a radiology report, a surgical agent develops a surgical plan, and a pathology\nagent examines tissue samples or blood test results to provide cellular- or molecular-level diagnostic insights. After\neach agent completes their assigned sub-tasks, a collaborative mechanism consolidates their diagnostic inputs into\na comprehensive medical opinion, which is then communicated back to the general practitioner agent [227, 228].\nAdditionally, during agent collaboration, shared data and insights contribute to improved overall diagnostic recom-\nmendations.However,effectivecollaborationamongmultipleagentsrequireslow-latencycommunication,especially"},"Question":"How do medical agents simulate departmental experts to deliver comprehensive diagnostic support?","Tool":"similarity_search"},{"index":7,"Answer":{"source":"Papers\/From Text to Multimodality_ Exploring the Evolution and Impact of Large Language Models in Medical Practice.pdf (page 6)","content":"clinical guidelines is essential for the effectiveness of Large ditionalprivacy concerns[3]. Developingsecure data storage,\nLanguage Models (LLMs) in complex medical tasks [81]. de-identification techniques, and access control mechanisms\nA Significant challenge in deploying LLMs for healthcare are crucial areas for future research. These challenges are\nis addressing the issues of hallucinations and bias. LLMs particularlyevidentinspecificmedicalfields,suchasdentistry,\ncan generate factually incorrect information and perpetuate where the use of LLMs requires robust safeguards to protect\nbiases present in their training data, which is particularly patient data [33], [100], [101].\nconcerning in medical contexts. To tackle this problem, [96] Additionalresearch is neededto define the optimalbalance\nintroduces Med-HALT, a benchmark and dataset specifically between human oversight and LLM autonomy, and establish-\ndesigned to evaluate and mitigate hallucinations in medical ing robust governance structures for LLMs in healthcare is\nLLMs. This tool emphasizes the critical need to address essential to ensure accountability and public trust. A frame-\nthese issues for safer healthcare applications. Additionally, work for evaluating LLMs in healthcare, including a gover-\n[97] underscorestheimportanceof incorporatingdiversereal- nance layer to ensure accountability and public confidence,\nworld data and domain-specific knowledge to reduce factual has been proposed [41]. Clear guidelines and standards are\ninaccuraciesandimprovethemodel’sgroundinginreal-world neededfordatagovernance,modeldevelopment,performance\nclinical scenarios. evaluation, bias mitigation, and transparency. A collaborative\nThe developmentof multilingualmodelsrepresentsanother approachinvolvingdevelopers,clinicians,ethicists,regulators,\ncrucial area for advancement in medical LLMs. Most LLMs and patients is vital for establishing trust and promoting the"},"Question":"What is a significant challenge in deploying Language Models for healthcare?","Tool":"similarity_search"},{"index":8,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 2)","content":"hyperintense areas, which typically signify acute bleeding. The second image focuses closer on the brainstem and cerebellum, also showing no\nevidence of swelling, and the structures of the posterior fossa appear well-preserved and normal in size. The third image is an axial slice at the level of\nthe lateral ventricles, presenting clear and symmetric ventricles, indicating the absence of hydrocephalus. The fourth ventricle is visible and appears\nnormal, suggesting a regression of previously noted hydrocephalus.\nFigure1: Constructingimagecaptionsinvariousapproaches.Detailedexplanationsofthesemethods\naregiveninAppendixF.Weusegpt-4astheLLMandgpt-4V astheMLLM.Strikethroughtexts\nindicateerroneousdescriptionsordescriptionsunrelatedtotheimage. Thiscaseissourcedfroma\nPubMedpaperathttps:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC2852039\/.\nCase Analysis Figure 1 presents examples generated by these methods. It can be ob-\nserved that Native-Caption captions are ambiguous and contain content unrelated to the image.\nLLM-Reformatted misinterpretsthreesub-imagesasaCTslide,leadingtomisleadingdescriptions,\nandfailstoexcludeirrelevantcontent. GPT4v-Distill generatesfactuallyincorrectdescriptionsdue\ntothelackofcontextualtext. Incontrast, MLLM-Reformatted producessuperiordescriptionsby\nleveragingbothvisualinformationandcontextualcues. Itaccuratelyandthoroughlydescribesthe\nkeyinformationoftheimage. ThesubsequentexperimentinSection4.3furtherdemonstratesthe\nhigherdataqualityofMLLM-Reformatted.\n3"},"Question":"\"What do hyperintense areas typically signify in medical imaging?\"","Tool":"similarity_search"},{"index":9,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 25)","content":"[59] ShukangYin,ChaoyouFu,SiruiZhao,KeLi,XingSun,TongXu,andEnhongChen. Asurveyonmultimodallargelanguagemodels.\nNationalScienceReview,pagenwae403,112024.\n[60] KaranSinghal,ShekoofehAzizi,TaoTu,SSaraMahdavi,JasonWei,HyungWonChung,NathanScales,AjayTanwani,HeatherCole-Lewis,\nStephenPfohl,etal.Largelanguagemodelsencodeclinicalknowledge.Nature,620(7972):172–180,2023.\n[61] CanwenXu,DayaGuo,NanDuan,andJulianMcAuley. Baize:Anopen-sourcechatmodelwithparameter-efficienttuningonself-chat\ndata. InProceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages6268–6278.Associationfor\nComputationalLinguistics,December2023.\n[62] HaochunWang,ChiLiu,NuwaXi,ZewenQiang,SendongZhao,BingQin,andTingLiu.Huatuo:Tuningllamamodelwithchinesemedical\nknowledge.arXivpreprintarXiv:2304.06975,2023.\n[63] TianyuHan,LisaCAdams,Jens-MichalisPapaioannou,PaulGrundmann,TomOberhauser,AlexanderLöser,DanielTruhn,andKenoK\nBressem. Medalpaca–anopen-sourcecollectionofmedicalconversationalaimodelsandtrainingdata. arXivpreprintarXiv:2304.08247,\n2023.\n[64] ChaoyiWu,WeixiongLin,XiaomanZhang,YaZhang,WeidiXie,andYanfengWang. Pmc-llama:towardbuildingopen-sourcelanguage\nmodelsformedicine.JournaloftheAmericanMedicalInformaticsAssociation,31:1833–1843,2024.\n[65] AugustinToma,PatrickRLawler,JimmyBa,RahulGKrishnan,BarryBRubin,andBoWang.Clinicalcamel:Anopen-sourceexpert-level\nmedicallanguagemodelwithdialogue-basedknowledgeencoding.arXivpreprintarXiv:2305.12031,2023.\n[66] HongboZhang,JunyingChen,FengJiang,FeiYu,ZhihongChen,GuimingChen,JianquanLi,XiangboWu,ZhangZhiyi,QingyingXiao,\nXiangWan,BenyouWang,andHaizhouLi.HuatuoGPT,towardstaminglanguagemodeltobeadoctor.InFindingsoftheAssociationfor\nComputationalLinguistics:EMNLP2023,pages10859–10885.AssociationforComputationalLinguistics,December2023.\n[67] ChengPeng,XiYang,AokunChen,KalebESmith,NimaPourNejatian,AnthonyBCosta,CherylMartin,MonaGFlores,YingZhang,\nTanjaMagoc,etal.Astudyofgenerativelargelanguagemodelformedicalresearchandhealthcare.NPJDigitalMedicine,6(1):210,2023."},"Question":"What is the focus of the discussed models in terms of their application to medicine?","Tool":"similarity_search"},{"index":10,"Answer":{"source":"Papers\/From Text to Multimodality_ Exploring the Evolution and Impact of Large Language Models in Medical Practice.pdf (page 10)","content":"A. Alaa, “Evaluating large language models as agents in the clinic,” Blood,vol.142,no.Supplement1,pp.2328–2328, nov22023.\nnpjDigitalMedicine, vol.7,no.1,apr32024. [94] R. Armitage, “Large language models must serve clinicians, not the\n[74] A. Mihalache, R. S. Huang, D. Mikhail, M. M. Popovic, R. Shor, reverse,” TheLancetInfectious Diseases,vol.24,no.5,pp.453–454,\nA.Pereira,J.Kwok,P.Yan,D.T.Wong,P.J.Kertesetal.,“Interpre- 52024.\ntationofclinicalretinalimagesusinganartificialintelligencechatbot,” [95] G. Briganti, “A clinician’s guide to large language models,” Future\nOphthalmology Science, p.100556,2024. Medicine AI,aug172023.\n[75] A.Sharma,A.Saxena,A.Kumar,andD.Singh,“Depressiondetection [96] LogeshKumarUmapathi, AnkitPal, andMalaikannan Sankarasubbu,\nusingmultimodalanalysiswithchatbotsupport,”in20242ndInterna- “Med-HALT:MedicalDomainHallucination TestforLargeLanguage\ntional Conference on Disruptive Technologies (ICDT). IEEE, 2024, Models,” Conference on Computational Natural Language Learning,\npp.328–334. 2023.\n[76] Franc¸ois Remy, Kris Demuynck, and Thomas Demeester, “Biolord- [97] GuangyuWang,GuoxingYang,ZongxinDu,LongjunFan,andXiaohu\n2023: Semantic Textual Representations Fusing LLM and Clinical Li, “Clinicalgpt: Large Language Models Finetuned with Diverse\nKnowledge GraphInsights,”arXiv.org,2023. Medical DataandComprehensive Evaluation,” arXiv.org,2023.\n[77] Q.Niu,J.Liu,Z.Bi,P.Feng,B.Peng,andK.Chen,“Largelanguage [98] L.Luo,J.Ning,Y.Zhao,Z.Wang,Z.Ding,P.Chen,W.Fu,Q.Han,\nmodelsandcognitivescience:Acomprehensivereviewofsimilarities, G.Xu,Y.Qiu,D.Pan,J.Li,H.Li,W.Feng,S.Tu,Y.Liu,Z.Yang,\ndifferences, andchallenges,” arXivpreprintarXiv:2409.02387, 2024. J. Wang, Y. Sun, and H. Lin, “Taiyi: a bilingual fine-tuned large\n[78] B. Steurer, Q. Vanhaelen, and A. Zhavoronkov, “Multimodal trans- languagemodelfordiversebiomedicaltasks,”JournaloftheAmerican\nformers and their applications in drug target discovery for aging and MedicalInformaticsAssociation,vol.31,no.9,pp.1865–1874,feb29"},"Question":"What is the purpose of using multimodal transformers and their applications in drug target discovery for aging?","Tool":"similarity_search"},{"index":11,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 4)","content":"A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine\nFlamingo’srobustmultimodalin-contextlearning(ICL)andfew-shotcapabilitiesmarkitastheGPT-3momentforthe\nmultimodaldomain,positioningitasthebeginningofMLLMs.TheseMLLMsemployLLMsascognitiveengines,\npreservingtheirinherentcapabilitieswhileintegratingpowerfulvisualfunctionalities[55].Thisadvancementpaves\nthewayforthedevelopmentofgeneralistmedicalAIsystems.Forexample,Med-PaLMM[21]achievedperformance\nlevelscomparabletoorexceedingthestate-of-the-artexpertmodelsacross14differentmedicaltasks,showcasingthe\npotentialofMLLMsasgeneralmedicalassistants.\n2.5. High-qualityData\nA key factor behind the success of LLMs and MLLMs is their use of large-scale training data, which enables\ntheacquisitionofuniversalrepresentationstransferabletodiverselanguageunderstandingandgenerationtasks[38].\nHowever,muchofthistrainingdatacomesfromwebsourceslikeWebText[47]andCommonCrawl,anditisinevitable\nthattherearesometoxicitiesandbiasesintheselargeamountsofwebdata,whicharealsocarriedovertoLLMsand\nMLLMs[25].Tomitigatetheadverseeffectsoflarge-scaledatasetsandimprovemodelperformance,researchersoften\nemployhigh-qualitydatasetsforfine-tuning.\nForexample,Lietal.[18]utilizedGPT-4tocreateopen-endedinstruction-followingdataderivedfrombiomedical\nimage-captiondatasets,subsequentlytrainingLLaVA-Medonthisdata.LLaVA-Medexhibitedremarkablemultimodal\nconversationalcapabilities,adeptlyaddressingbiomedicalimagequeriesbasedonuserinstructions.Tofurtherrefine\nmultimodal medical datasets, Xie et al. [56] introduced a dataset enriched with multi-grained annotations. These\nannotations encompass global context, modality details, and localized descriptions of medical images. Training\nLLaVA-Med on this dataset led to a nearly 10% average performance boost across three biomedical VQA datasets,\nemphasizingthecriticalroleofhigh-qualitydatasets.Notably,LIMA[38]fine-tunedusingonly1,000meticulously"},"Question":"What is the role of high-quality datasets in refining multimodal medical models?","Tool":"similarity_search"},{"index":12,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 1)","content":"Xiao et al.: PreprintsubmittedtoElsevier Page 2 of 32"},"Question":"What is the main topic of the study submitted to Elsevier?","Tool":"similarity_search"},{"index":13,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 28)","content":"Costa,MonaGFlores,etal.Alargelanguagemodelforelectronichealthrecords.NPJDigitalMedicine,5(1):194,2022.\n[154] RuixiangTang,XiaotianHan,XiaoqianJiang,andXiaHu.Doessyntheticdatagenerationofllmshelpclinicaltextmining?arXivpreprint\narXiv:2303.04360,2023.\nXiao et al.: PreprintsubmittedtoElsevier Page 29 of 32"},"Question":"Does synthetic data generation of LLMs help clinical text mining?","Tool":"similarity_search"},{"index":14,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 16)","content":"For example, during medical consultations, doctors typically record key information from patient interactions,\nwhich serves as a basis for evaluating conditions or informing other clinical reports. Medical LLMs can function\nas clinical note-taking tools, automating this task for doctors [65]. Doctors can provide LLMs with doctor-patient\ninteraction records, which the models process to generate detailed medical notes [174]. Doctors can also prompt\nLLMs to simplify medical notes by removing complex details and generating concise summaries for easier review\nandanalysis[19].Followingamedicaldiagnosis,doctorsoftendraftdiagnosticdocumentssuchasradiologyreports.\nMedical MLLMs, capable of processing visual inputs, are particularly effective in assisting with radiology report\ngeneration. For example, miniGPT-Med, developed by Alkhaldi et al. [118], achieved state-of-the-art performance\nin generating medical reports, surpassing prior models in accuracy by 19%. This underscores the feasibility of\nusing medical MLLMs for radiology report generation. During treatment, doctors explain the cause of the disease,\nthe treatment process, and provide detailed clinical information to patients through clinic letters. By utilizing\nLLMs to generate clinic letters, clinicians can streamline this tedious process, with the resulting letters exhibiting\ncoherence, accuracy, and empathy comparable to those created by humans [175]. After patient recovery, clinicians\nallocatesignificanttimetodraftingdischargesummaries,potentiallydelayingpatientdischarge.ByemployingLLMs,\nclinicianscangeneratecompletedischargesummariesinsecondsbyprovidingatemplateandnecessaryinputs[176].\nThequalityofthesesummariesoftensurpassesthoseproducedbyjuniordoctors[177].\nLeveraging advanced LLMs and MLLMs, various clinical reports from patient admission to discharge can be\nautomaticallygenerated.Thesereportsaremorecomprehensiveandaccuratethanthoseproducedbyhumans[19,177],"},"Question":"What is the purpose of automating tasks for doctors?","Tool":"similarity_search"},{"index":15,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 1)","content":"A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine\n...... ......\nResearch Paper Literature\nWeb Page Radiology Report\nBook EHR\nAutomatic Evaluation Metrics\nPre-Trainig Fine-Tuning Evaluation\nHuman Evaluation\nTransformer General LLM \/ MLLM Medical LLM \/ MLLM AI Evaluation\nFig. 1. The process of constructing and evaluating medical LLMs and MLLMs.\nhospitals.Third,giventheuniquecharacteristicsofthemedicalfield,itisessentialnotonlytoevaluatetheperformance\nof LLMs and MLLMs on benchmarks but also to assess their instruction-following ability [27, 5, 11], safety, and\nethicalconsiderations,necessitatingadditionaltrainingandevaluationstrategiestoenhanceandmeasurethemodels’\nperformance across multiple dimensions. Furthermore, the development of LLMs and MLLMs in the medical field\nis still in its infancy, with many of their potential application scenarios remaining undefined. Moreover, they face a\nrangeofchallenges,includinghallucinations[28,29,30]andalackofup-to-dateinformation[12],whichsignificantly\nimpedetheirpracticaluseinclinicalsettings.\nTo address the aforementioned challenges, this survey begins by tracing the evolution of LLMs and MLLMs\nthrough the lens of paradigm shifts. Subsequently, it reviews existing medical LLMs and MLLMs, summarizing\ntheirstructuralcharacteristics.ThesurveythengathersdatasetssuitablefortrainingmedicalLLMsandMLLMsand\nelaborates on methods for training and evaluating these models, as shown in Fig. 1. Furthermore, to highlight the\nsignificantpotentialimpactofLLMsandMLLMsinmedicine,thissurveysummarizestheirapplicationsinclinical\npracticeandanalyzescurrentlimitationsandpotentialsolutions.Finally,thesurveyexploresthefuturedirectionsof\nmedicalLLMsandMLLMs,offeringforward-lookingandinsightfulperspectives.\nMedicineisamultimodalfield[31,32],makingthestudyofmedicalMLLMsparticularlyimportant,astheycan\nintegrateandanalyzeinformationfromvariousmodalitiestoenhanceclinicaldecisionsupport,diseasediagnosis,and"},"Question":"What is the main purpose of tracing the evolution of LLMs and MLLMs through paradigm shifts in this survey?","Tool":"similarity_search"},{"index":16,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 3)","content":"filteringpipeline: (1)TextFiltering. Amedicalvocabularywasusedtofilteroutdatawherethe\ncontextaltextcontainsainsufficientnumberofmedicalterms. (2)ImageFiltering. Weexcluded\nlow-resolutionimages(lessthan336x336pixels). Amedicalimageclassificationmodel,trainedon\n1Kmanuallylabeledimagesand10KMLLM-labeledimages,isusedtoidentifymedicalimages. (3)\nDeduplication. UsingSentence-BERT[17]astheencoder,weobtainedsemanticembeddingsof\ntheimagecaptionsandfilteredoutimageswithoverlysimilarcontexts. Formoredetails,pleasesee\nAppendixB.\nUltimately,wefilteredout914,960medicalimagesandtheirassociatedcontextualtext(captionsand\ninlinementions). Figure3illustratesthediversityofmedicalmodalitiesandimageregionscovered\nbyPubMedVision’simages. Thesemedicalimagesarethenusedtosequentiallyconstruct1.3million\nVQAdatapointsformedicalalignment.\nFigure3: ImageDiversityinPubMedVision. Arandomsampleof500imagesfromPubMedVision\narecategorized. Left: Distributionofbodypartsdepictedintheimages. Right: Distributionof\nimagingmodalities.\n4"},"Question":"What is the purpose of filtering out medical images and their associated contextual text?","Tool":"similarity_search"},{"index":17,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 22)","content":"inurgentscenarioslikesurgicalassistance,wherereal-timeinteractionisessential.Consequently,6Gcommunication\ntechnologycanalsooffervitaltechnicalsupportforenablingseamlessmulti-agentcollaboration.\n7.3. GeneralMedicalAssistant\nAlthough medical MLLMs currently support visual and text modalities, enabling tasks such as medical image\nreport generation and visual question answering, clinical environments demand these models to process additional\ninput modalities, including time series and audio data [229] and audio data. These additional input modalities can\nprovidemorecomprehensivepatientinformation,particularlyplayingacrucialroleindynamicmonitoringandreal-\ntimediagnosis.\nAtpresent,medicalLLMsandMLLMsprimarilyrelyonstaticdata,suchasmedicalrecordsandimagingstudies,\nlimitingtheircapacitytoaccountforpatients’dynamicdata.IncorporatingtimeseriesdataallowsmedicalMLLMs\ntoanalyzehealthtrendsanddetectearlysignsofdiseaseprogression,leadingtomoreprecisealertsandpredictions.\nFor example, in an ICU, in an ICU, medical MLLMs can integrate continuous ECG monitoring and blood pressure\nvariations to predict cardiac arrest risks and recommend timely interventions. Similarly, audio data, such as cough\nrecordings, can aid in diagnosing lung diseases or predicting respiratory failure risks. Voice analysis by medical\nMLLMs can also assess a patient’s emotional state, supporting psychiatrists in diagnosing mental health conditions\nlikedepressionandanxiety[230,231,232].\nBeyond additional input modalities, medical MLLMs are also expected to support a broader range of output\nmodalities.Currently,mostmedicalMLLMsrelyonLLMcomponentsasoutputmodules,restrictingoutputstotext.In\nclinicalpractice,medicalMLLMsrequirevisualoutputmodulesforregion-andpixel-leveloutputs,suchassegmenting\npathologicalareasinimagesaccordingtophysicianinstructionstodelivermoredetaileddiagnosticevidence.\n8. Conclusion\nIn recent years, advancements in LLMs have driven significant breakthroughs in NLP, enabling researchers to"},"Question":"What is the significance of integrating additional input modalities in medical MLLMs?","Tool":"similarity_search"},{"index":18,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 6)","content":"+ PubMedVision 61.0 58.8 50.0 44.7 38.7 49.1\nHuatuoGPT-Vision-34B 64.6 62.5 50.6 54.1 44.2 54.4\nTable4: ResultsonthetestsetfortheMMMUHealth&Medicinetrack. TheHealth&Medicine\ntrackisdividedintofivecategories: BMSforBasicMedicalScience,CMforClinicalMedicine,\nDLMforDiagnosticsandLaboratoryMedicine,PforPharmacy,andPHforPublicHealth. Results\nareobtainedbysubmittingtotheofficialwebsite.\nMMMUHealth&MedicineTrack MMMUisawidelyrecognizedmultimodalbenchmark,and\nweutilizeitsHealth&MedicineTrackforassessment. FigureTable4presentstheresultsofthe\nMMMUtestset,showingthatLLaVA-v1.5-LLaMA3-8B+PubMedVisionsurpassedothermodelsin\ntheHealth&MedicineTrack,withperformancecomparabletothelarger-parameterLLaVA-v1.6-34B.\nThesefindingsfurthervalidatePubMedVision’seffectivenessinaligningmedicalimages.\nApplicability of PubMedVision To verify the applicability of PubMedVision across different\nMLLMmodels,wefurthertrainedPubMedVisiononotherMLLMmodels,specificallyLLaVA-v1.5-\n7BandQwen-VL-Chat. AsdemonstratedinTable5,PubMedVisioneffectivelyenhancesthemedical\nmultimodalcapabilitiesofthesediverseMLLMmodelsaswell.\n7"},"Question":"What is the main purpose of utilizing the MMMU Health&Medicine Track for assessment?","Tool":"similarity_search"},{"index":19,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 21)","content":"Sample5:DoctorandPatient’sFamily\nDescription The image displays a series of axial cuts of computed tomography (CT) scans from\nthe thorax and abdomen. In the thoracic region, there are multiple tiny nodules and\ncysticlesionsvisibleinbothlungs. Asignificantleftpleuraleffusionisevidentwith\nsignsofpneumothorax,characterizedbythepresenceofairinthepleuralspacewhich\ncanbeidentifiedbythevisceralpleurallineandaclearspacedevoidoflungmarkings.\nAdditionally,thereisevidenceofpericardialeffusionasseenbytheabnormalaccumulation\noffluidaroundtheheart. TheCTscansoftheabdomenshowenlargementofmultiple\nlymphnodesandmultiplehepaticmasses,whichappearasirregularareaswithintheliver.\nQA-query Doctor,canyouexplainwhatallthesefindingsmean?Howseriousaretheseconditions\nandwhatarethepossibletreatments?\nQA-answer Certainly,Iunderstandyourconcerns. Thefindingsindicateafewseriousconditions.\nMultipletinynodulesandcysticlesionsinthelungscouldbeindicativeofaninfectious\nor inflammatory process, or less likely, metastatic disease. The large amount of left\npleuraleffusionandthepresenceofpneumothoraxareconcerningastheycancompromise\nlungfunction. Pleuraleffusionmeansthereisfluidaccumulationthatcanrestrictlung\nexpansion,andpneumothoraxinvolvesairleakingintothespacebetweenthelungand\nchestwall,whichcancausethelungtocollapse.Pericardialeffusion,thefluidaroundthe\nheart,canaffectheartfunctionifsevere.Theenlargedlymphnodesandhepaticmassesin\ntheabdomenmightsuggestapossiblemalignancyormetastaticspread.Treatmentoptions\nwouldvarybasedontheunderlyingcausebutcouldincludedrainageoffluidfromthe\npleuralandpericardialspaces,antibioticsorchemotherapyifinfectionormalignancyare\nconfirmed,andpotentiallysurgerytoaddressthehepaticmasses.Amultidisciplinaryteam\napproachwillbecrucialinmanagingtheseconditionseffectively.\nTable14: Sample5forDoctorandPatient’sFamilySenorio.\n22"},"Question":"Can you explain what all these findings mean? How serious are these conditions and what are the possible treatments?","Tool":"similarity_search"},{"index":20,"Answer":{"source":"Papers\/From Text to Multimodality_ Exploring the Evolution and Impact of Large Language Models in Medical Practice.pdf (page 6)","content":"Additionally, ensuring data privacy and obtaining informed accessibility and applicability in diverse linguistic contexts.\nconsent are critical ethical considerations that require careful The potential of bilingual fine-tuned LLMs, such as Taiyi,\nattention, particularly when dealing with sensitive medical can achieve superior performance on biomedical NLP tasks\ninformation [35]. compared to general LLMs [98]. However, more research is\nneeded to develop effective methodsfor creating and evaluat-\nB. InterdisciplinaryCollaborationandKnowledgeIntegration\ning multilingual medical LLMs that can cater to the needs of\n1) FosteringEffectiveInterdisciplinaryCollaboration: The diverse patient populations [84], [99].\ndevelopment of clinically relevant and useful MultiModal\nLarge Language Models (MLLMs) requires bridging the gap\nD. Ethical and Regulatory Framework\nbetweencomputerscienceandmedicine.Thisinterdisciplinary\nchallengecallsforcollaborationamongmedicalprofessionals,\nThe potential of MLLMs in healthcare is clear, but their\ndata scientists, ethicists, and policymakers [65], [89], [90].\ndeployment in real-world clinical settings presents significant\nSuchcollaborationisessentialtofosterasharedunderstanding\nethical and regulatory challenges that demand careful consid-\nof both the technical capabilities of LLMs and the specific\neration and further research.\nneeds and constraints of the healthcare domain.\nA key issue is the lack of clear guidelines and regulations\nAs LLMs become more integrated into healthcare work-\nspecifically tailored for the development, deployment, and\nflows, it is crucial to define the roles and responsibilities\nevaluation of LLMs in healthcare [87], [89]. Existing frame-\nof various stakeholders [91]. [10] stresses the importance of\nworksformedicalAImaynotfullyaddresstheuniqueethical\nincentivizing users, developers, providers, and regulators to\nand legal implications of LLMs, especially in the context of\nprepareforthetransformativeroleofLLMsinevidence-based"},"Question":"What is a critical ethical consideration that requires careful attention when dealing with sensitive information?","Tool":"similarity_search"},{"index":21,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 14)","content":"D PromptsfordifferentQAscenarios\nInourstudy,Instruction-TuningVQAisgeneratedbasedontenpre-setdifferentscenarios. This\napproachcoversabroaderrangeofmedicaltopicsandscenarios,therebyenhancingthediversityof\ntheVQApairs,andmorecomprehensivelyimprovingtheabilitytofollowinstructions. Thesampling\nmethod also prevents the overconcentration or absence of certain scenarios, contributing to data\nbalance,whichinturnimprovestheperformanceandstabilityofthemodel.\nStandardQ&A\nYou need to generate a question-and-answer pair based on this image. The\nquestion should be designed to test other models’ understanding of this\nmedical image; it should be phrased simply and conversationally. However,\nyour response should be professional, showcasing your understanding of\nthe medical image by providing useful information derived from the image\nand detailed analysis. The reply should offer detailed and rich useful\ninformation.\nFigure8: PromptforStandardQ&AScenario: Aguideforcraftingastandardquestion-and-answer\nscenario.\nAIModelAssistingDoctor\nYou need to generate a question-and-answer pair based on this image. You\nneed to act as a doctor using an AI model to analyze a medical image\nto better understand a patient’s condition. The doctor should ask\nspecific questions about structures, abnormalities, and potential clinical\nsignificance visible on the image. The AI model should provide detailed\nanalyses based on its algorithms but not make final clinical diagnoses.\nThe doctor will use the information provided by the AI model to aid their\ndiagnostic decision-making process.\nFigure9: PromptforAIModelAssistingDoctorScenario: Asimulateddialoguewhereadoctor\nconsultsanAImodelaboutdetailsinamedicalimagetoimprovediagnosticaccuracy.\nAIModelAssistingPatient\nYou need to generate a question-and-answer pair based on this image. You\nneed to act as an AI model interacting with a patient who has questions\nabout visible content on their medical image. The patient may be curious"},"Question":"What is the main purpose of enhancing diversity in VQA pairs?","Tool":"similarity_search"},{"index":22,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 7)","content":"3.2.2. LLMBackbone\nAs the cognitive engine of MLLMs, the LLM backbone is the most critical module among the three primary\ncomponentsandcontainsthelargestnumberofparameters.ItequipsMLLMswithcapabilitiessuchastextinteraction,\nICL,andreasoning.TheoperationalprincipleoftheLLMbackboneinMLLMsisillustratedbelow:\n𝑅=𝐿(𝐻 ,𝑇 ) (3)\n𝑥 𝑥\nWhere𝐿representstheLLMbackbone,𝑅denotestheresponseoutputoftheLLM,𝑇 indicatestheembedded\n𝑥\ntokensofthetextinput,and𝐻 arevisualrepresentationsthatLLMcanunderstand.Thespecificmeaningof𝐻 is\n𝑥 𝑥\nexplainedinEquation(4).\nAlthoughpowerfulLLMslikeChatGPThavenotyetbeenopen-sourced,numeroushigh-qualityopen-sourceLLMs\nareavailableforresearchers.Amongthese,theLLaMAseries[6,7,8]developedbyMeta,standsoutasoneofthemost\npopularopen-sourceLLMsandisfrequentlyusedasthebackboneforMLLMs.Additionally,fine-tunedversionsof\nLLaMA,suchasVicuna-13B[99],achieveperformancecomparableto90%ofChatGPTandBard.Notably,different\nmodels demonstrate varying levels of performance across languages. For example, Mistral [100] excels in French,\nQwen[101]isoptimizedforChinese,andGPT-4offersrobustsupportforabroaderrangeoflanguages.Consequently,\nresearcherscanchooseLLMbackbonesbasedonspecificlinguisticrequirements.\n3.2.3. ModalityAlignment\nAlthoughintegratingavisionencoderintoLLMsenablesthemtoprocessvisualinputs,LLMstrainedexclusively\non text datasets cannot interpret the output features 𝑍 produced by the vision encoder. Consequently, modality\n𝑥\nalignmentisrequiredtotransform𝑍 intoaformatthatLLMscanunderstand,asshowninEquation(4):\n𝑥\n𝐻 =𝑓(𝑍 ) (4)\n𝑥 𝑥\nWhere 𝑓 represents the modality alignment method, and 𝐻 refers to visual representations that LLMs can\n𝑥\nunderstand. Modality alignment plays a critical role in enabling MLLMs to interpret visual information and sig-\nnificantly enhances their multimodal capabilities. The subsequent sections introduce four established modality\nalignmentmethods:GATEDXATTN-DENSELayers,Query-Basedmethod,Projection-Basedmethod,andPrompt\nAugmentation."},"Question":"\"What is the purpose of the most critical module among the three primary components in MLLMs?\"","Tool":"similarity_search"},{"index":23,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 24)","content":"hallucinationinnaturallanguagegeneration.ACMComputingSurveys,55(12):1–38,2023.\n[31] QikaLin,YifanZhu,XinMei,LingHuang,JingyingMa,KaiHe,ZhenPeng,ErikCambria,andMenglingFeng.Hasmultimodallearning\ndelivereduniversalintelligenceinhealthcare?acomprehensivesurvey.InformationFusion,page102795,2024.\n[32] Felix Krones, Umar Marikkar, Guy Parsons, Adam Szmul, and Adam Mahdi. Review of multimodal machine learning approaches in\nhealthcare.InformationFusion,114:102690,2025.\n[33] KaiHe,RuiMao,QikaLin,YuchengRuan,XiangLan,MenglingFeng,andErikCambria.Asurveyoflargelanguagemodelsforhealthcare:\nfromdata,technology,andapplicationstoaccountabilityandethics.arXivpreprintarXiv:2310.05694,2023.\n[34] BenyouWang,QianqianXie,JiahuanPei,ZhihongChen,PrayagTiwari,ZhaoLi,andJieFu. Pre-trainedlanguagemodelsinbiomedical\ndomain:Asystematicsurvey.ACMComputingSurveys,56(3):1–52,2023.\n[35] SurendrabikramThapaandSurabhiAdhikari. Chatgpt,bard,andlargelanguagemodelsforbiomedicalresearch:opportunitiesandpitfalls.\nAnnalsofBiomedicalEngineering,51(12):2647–2651,2023.\n[36] JesutofunmiAOmiye,HaiwenGui,ShawheenJRezaei,JamesZou,andRoxanaDaneshjou. Largelanguagemodelsinmedicine:the\npotentialsandpitfalls:anarrativereview.AnnalsofInternalMedicine,177(2):210–220,2024.\n[37] RajeshBhayana. Chatbotsandlargelanguagemodelsinradiology:Apracticalprimerforclinicalandresearchapplications. Radiology,\n310(1):e232756,2024.\n[38] ChuntingZhou,PengfeiLiu,PuxinXu,SrinivasanIyer,JiaoSun,YuningMao,XuezheMa,AviaEfrat,PingYu,LiliYu,etal. Lima:Less\nismoreforalignment.AdvancesinNeuralInformationProcessingSystems,36:55006–55021,2024.\n[39] PengfeiLiu,WeizheYuan,JinlanFu,ZhengbaoJiang,HiroakiHayashi,andGrahamNeubig. Pre-train,prompt,andpredict:Asystematic\nsurveyofpromptingmethodsinnaturallanguageprocessing.ACMComputingSurveys,55(9):1–35,2023.\n[40] HuanLiuandLeiYu. Towardintegratingfeatureselectionalgorithmsforclassificationandclustering. IEEETransactionsonKnowledge\nandDataEngineering,17(4):491–502,2005."},"Question":"Does multimodal learning lead to universal intelligence in healthcare?","Tool":"similarity_search"},{"index":24,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 29)","content":"[163] Chin-YewLin. Rouge:Apackageforautomaticevaluationofsummaries. InTextSummarizationBranchesOut,pages74–81.Association\nforComputationalLinguistics,2004.\n[164] YonghuiWu,MikeSchuster,ZhifengChen,QuocVLe,MohammadNorouzi,WolfgangMacherey,MaximKrikun,YuanCao,QinGao,\nKlausMacherey,etal.Google’sneuralmachinetranslationsystem:Bridgingthegapbetweenhumanandmachinetranslation.arXivpreprint\narXiv:1609.08144,2016.\n[165] JiweiLi,MichelGalley,ChrisBrockett,JianfengGao,andBillDolan. Adiversity-promotingobjectivefunctionforneuralconversation\nmodels. InProceedingsofthe2016ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:Human\nLanguageTechnologies,pages110–119.AssociationforComputationalLinguistics,2016.\n[166] RamakrishnaVedantam,C.LawrenceZitnick,andDeviParikh. Cider:Consensus-basedimagedescriptionevaluation. In2015IEEE\nConferenceonComputerVisionandPatternRecognition,pages4566–4575,2015.\n[167] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In\nInternationalConferenceonLearningRepresentations,2020.\n[168] JiaanWang,YunlongLiang,FandongMeng,ZengkuiSun,HaoxiangShi,ZhixuLi,JinanXu,JianfengQu,andJieZhou. IsChatGPTa\ngoodNLGevaluator?apreliminarystudy. InProceedingsofthe4thNewFrontiersinSummarizationWorkshop,pages1–11.Association\nforComputationalLinguistics,December2023.\n[169] LianminZheng,Wei-LinChiang,YingSheng,SiyuanZhuang,ZhanghaoWu,YonghaoZhuang,ZiLin,ZhuohanLi,DachengLi,EricP.\nXing,HaoZhang,JosephE.Gonzalez,andIonStoica. Judgingllm-as-a-judgewithmt-benchandchatbotarena. InAdvancesinNeural\nInformationProcessingSystems,2024.\n[170] PeterSzolovits,RameshSPatil,andWilliamBSchwartz.Artificialintelligenceinmedicaldiagnosis.AnnalsofInternalMedicine,108(1):80–\n87,1988.\n[171] JYuan,PBao,ZChen,MYuan,JZhao,JPan,YXie,YCao,YWang,ZWang,etal.Advancedpromptingasacatalyst:Empoweringlarge\nlanguagemodelsinthemanagementofgastrointestinalcancers.TheInnovation,521,2023."},"Question":"\"What is the main purpose of the papers in this context?\"","Tool":"similarity_search"},{"index":25,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 22)","content":"make substantial progress toward artificial general intelligence by extending LLMs into the multimodal domain,\nresulting in the creation of MLLMs. Concurrently, the rapid development and impressive performance of general\nLLMsandMLLMshavespurredtheemergenceofnumerousmedicalLLMsandMLLMs.Thissurveyaimstohelp\nresearchersandmedicalpractitionersunderstandthetechnologicaladvancementsanddevelopmentalstatusofmedical\nLLMsandMLLMs.ItfocusesontheparadigmshiftofLLMsandMLLMs,highlightingtheirevolutionfromfeature\nengineeringtostructureengineering,objectiveengineering,andnowtopromptengineeringanddataengineering.The\nsurvey summarizes the mainstream architectures of current LLMs and MLLMs, compiles a list of existing medical\nLLMsandMLLMs,andprovidesinsightsintovariousarchitecturesandmodelcomponents.Additionally,itpresents\nXiao et al.: PreprintsubmittedtoElsevier Page 23 of 32"},"Question":"What is the main purpose of extending LLMs into the multimodal domain?","Tool":"similarity_search"},{"index":26,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 12)","content":"imageandcontextualtextdata,specificallyLLaVA-MedPMCdata(514K)[7],PMC-Inline(11M)\n[3],andPMC-OA(1M)[9]. Althoughthedatasetisextensive,mostofthedataconsistsofcharts\nandgraphsfrompapersratherthanmedicalimages. Therefore,weneedtofilterouthigher-quality\nmedicalimage-textdata. Weestablishedapipelineasfollows:\n1. ContextualTextFiltering: UtilizingtheSPECIALISTLexicon3fromtheUnifiedMedical\nLanguageSystem,weemployedGPT-4tofilteroutcommonphrases,creatingarefined\nmedicallexicon. Usingthislexicon,weassessedthenumberofmedicaltermsinimage\ncaptions,filteringoutdatawithfewerthanfivemedicalterms. Thisensuresthecaptionsare\nsufficientlyinformative.\n3https:\/\/www.nlm.nih.gov\/research\/umls\/new_users\/online_learning\/LEX_001.html\n13"},"Question":"What is the purpose of filtering out higher-quality medical image-text data?","Tool":"similarity_search"},{"index":27,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 26)","content":"recognitionatscale.InInternationalConferenceonLearningRepresentations,2021.\n[95] Xi Chen, Xiao Wang, Lucas Beyer, Alexander Kolesnikov, Jialin Wu, Paul Voigtlaender, Basil Mustafa, Sebastian Goodman, Ibrahim\nAlabdulmohsin,PiotrPadlewski,etal.Pali-3visionlanguagemodels:Smaller,faster,stronger.arXivpreprintarXiv:2310.09199,2023.\n[96] ShaurySrivastav,MercyRanjit,FernandoPérez-García,KenzaBouzid,ShruthiBannur,DanielC.Castro,AntonSchwaighofer,Harshita\nSharma,MaximilianIlse,ValentinaSalvatelli,SamBond-Taylor,FabianFalck,AnjaThieme,HannahRichardson,MatthewP.Lungren,\nStephanieL.Hyland,andJavierAlvarez-Valle.MAIRAatRRG24:Aspecialisedlargemultimodalmodelforradiologyreportgeneration.In\nProceedingsofthe23rdWorkshoponBiomedicalNaturalLanguageProcessing,pages597–602.AssociationforComputationalLinguistics,\nAugust2024.\n[97] MingYLu,BowenChen,DrewFKWilliamson,RichardJChen,MelissaZhao,AaronKChow,KenjiIkemura,AhrongKim,DimitraPouli,\nAnkushPatel,etal.Amultimodalgenerativeaicopilotforhumanpathology.Nature,634:466–473,2024.\n[98] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,XinggangWang,TiejunHuang,XinlongWang,andYueCao. Eva:Exploring\nthelimitsofmaskedvisualrepresentationlearningatscale. InProceedingsoftheIEEE\/CVFConferenceonComputerVisionandPattern\nRecognition,pages19358–19369,2023.\n[99] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,SiyuanZhuang,YonghaoZhuang,JosephE.\nGonzalez,IonStoica,andEricP.Xing.Vicuna:Anopen-sourcechatbotimpressinggpt-4with90%*chatgptquality,March2023.\n[100] AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSinghChaplot,DiegodelasCasas,FlorianBressand,\nGiannaLengyel,GuillaumeLample,LucileSaulnier,etal.Mistral7b.arXivpreprintarXiv:2310.06825,2023.\n[101] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,WenbinGe,YuHan,FeiHuang,etal. Qwentechnical\nreport.arXivpreprintarXiv:2309.16609,2023.\n[102] MichaelMoor,QianHuang,ShirleyWu,MichihiroYasunaga,YashDalmia,JureLeskovec,CyrilZakka,EduardoPontesReis,andPranav"},"Question":"What is the relationship between masked visual representation learning and scale?","Tool":"similarity_search"},{"index":28,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 22)","content":"Sample6:MedicalTeacherandStudent(MultipleImages)\nDescription TheprovidedimagesdepicttwoAP(anteroposterior)pelvisradiographs.Thefirstradio-\ngraphshowsbothhipswithsymmetricplacementofscrewsorpinsinwhatappearstobea\nbilateralsurgicalintervention,possiblyforhipstabilization.Thereisvisibledeformation\nofthepelvicbonesandtheacetabulae(hipsockets)showsignsofprotrusionintothe\npelviccavity.Thesecondimagefeaturesmorepronouncedskeletalchangesincludinga\nnotablymisalignedrightfemurwithsurgicalhardwareadjustments,possiblyindicatinga\npost-injuryorpost-surgicalstatewithacomplexfixationdeviceattachedtotherightside\nextendingfromthepelvistothefemur.Theseimagessuggestsignificantprogressionin\nthepatient’scondition,hintingatongoingissueswithhipstabilityandpelvicdeformation.\nQA-query Giventhesepelvisradiographs,canyoudescribeanyobservablechangesorindicatorsthat\nsuggestaprogressioninthepatient’spelviccondition?\nQA-answer Certainly,examiningtheradiographs,onecannoteanincreaseddeformationinthepelvic\nregiononthesecondimagecomparedtothefirst. Thisincludesgreaterprotrusionof\nacetabularregionsandamorecomplexassemblyofsurgicalhardwareontherightside,\nsuggestingincreasedintervention.Suchfindingslikelyindicateasignificantprogressionof\nunderlyingconditionsaffectingthepelvicstructurewhichnecessitatedadditionalsurgical\nmeasuresovertime.\nTable15: Sample6forMedicalTeacherandStudentSenorio. (MultipleImages)\nSample7:EvaluatorandAIModel(MultipleImages)\nDescription Theseimagesaremedicalultrasoundscansoftheeye. ImageAshowsanultrasound\nbiomicroscopy (UBM) scan depicting angle closure with a shallow anterior chamber\n(indicatedbythethickarrow)andanteriorrotationoftheciliaryprocesswithciliochoroidal\neffusion(thinarrow).ImageBisaB-scanultrasonographyshowingdiffusethickeningof\nthechoroid(asterisk).ImageC,takenonemonthaftertreatment,showsaUBMscanof\nthelefteyewithanormalanteriorchamberangleandthedisappearanceofciliochoroidal\neffusion.ImageDisanotherB-scanultrasonography,whichshowsnochoroidalthickening."},"Question":"Can you describe any observable changes or indicators that suggest a progression in the patient's pelvic condition?","Tool":"similarity_search"},{"index":29,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 1)","content":"treatmentplanning.However,thearticlesrelevanttothissurveymainlyfocusonmedicalLLMsandlackadetailed\nexamination of medical MLLMs [15, 33, 34]. Additionally, most articles focus on the applications and impacts of\nLLMs in medicine but lack detailed discussions of technical aspects [12, 35, 26, 36, 37], such as datasets, model\narchitectures,andconstructionmethods.Incontrast,thissurveynotonlyexaminesthebackgroundandprinciplesof\nLLMsandMLLMsbutalsoexplorestheirapplicationsandimpactsinmedicine,offeringaclearlogicalstructureand\nsubstantialdepthandbreadth.Insummary,thecontributionsofthissurveyareasfollows:\n• This survey offers a thorough overview of medical LLMs and MLLMs, starting with an examination of their\ndevelopmental background and architectural frameworks. Building on this foundation, it catalogs existing\nmedicalLLMsandMLLMswhileprovidingadetailedanalysisoftheirstructuralvariationsandkeycomponents.\n• This survey systematically elucidates the complete process of medical LLMs and MLLMs, from training to\nevaluation, covering fine-tuning methods, evaluation strategies, and relevant medical datasets. Additionally, it\nhighlightshowtoselectappropriatedatasets,fine-tuningmethods,andevaluationstrategiestoassistresearchers\nintherapiddevelopmentofmedicalLLMsandMLLMs.\n• Thissurveysummarizestheapplications,challenges,andpotentialsolutionsofmedicalLLMsandMLLMsin\nclinicalpractice,whileprovidingaforward-lookinganalysisoffuturedevelopmentaltrajectories.Itseekstooffer\nvisionaryperspectivesthatinspireadvancementsinthefield,benefitingmedicalprofessionalsandresearchers\nalike.\nThis survey aims to advance the development of LLMs and MLLMs for clinical medicine applications, thereby\npromotingdeeperintegrationbetweenartificialintelligenceandhealthcare.Thestructureofthissurveyisoutlinedin\nFig. 2: Section 2 reviews the development background of LLMs and MLLMs. Section 3 describes the architectures\nofcurrentLLMsandMLLMsandhighlightsthestructuraldifferencesamongthem.Section4coversthedatasetsfor"},"Question":"What is the main purpose of this survey?","Tool":"similarity_search"},{"index":30,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 17)","content":"F ComparisonofMethodsforConstructingMultimodalDatasets\nTable9presentsfourmethodsofsynthesizingmultimodaldata. Tofacilitateabettercomparison,\nweuniformlyconstructcaptionsusingthesefourmethods. Thesecaptionsarethencombinedwith\nthe query \"Please provide a description of the given medical image\" to form a VQA dataset for\ncomparingthedifferencesamongthevariousmethods.\nDataset CaptionSynthesisMethod\nNative-Caption-60K Uses the native contextual text (Caption and inline Mention) as the image\ncaption.\nLLM-Reformatted-60K FollowingthesynthesismethodofLLaVA-MedwithLLMs,weprovideonly\nthecontextualtexttoLLM(gpt-4-turbo-2024-04-09)toconstructanswers.For\nspecificprompts,seeFigure19.\nGPT4v-Distill-60K WeonlyprovidetheimagetoGPT-4-Vision(gpt-4-turbo-2024-04-09)togen-\nerateadescriptioninresponsetothequery\"Pleaseprovideadescriptionofthe\ngivenmedicalimage\".\nMLLM-Reformatted-60K ThemethodofPubMedVision,whereMLLMsconstructdatabasedoncon-\ntextualtextandvisualinformationfromtheimage.Weusetheanswersfrom\nPubMedVision’sAlignmentVQAastheconstructedcaption.\nTable9: Descriptionoffourmethodsforconstructingimagecaptions.\nPromptforLLM-Reformatted\nYou have been provided with textual context information of images from a\nbiomedical research paper, but you do not have access to the actual image.\nYou need to respond to the following question based on this image’s context\ninformation.\nIn your response, avoid using phrases like ‘mentioned’, ‘caption’, or\n‘context’. Instead, describe the information as if it were directly observed\n‘in the image’. Answer responsibly, avoiding any overconfidence, and refrain\nfrom giving medical advice or diagnostic information. Encourage the user to\nconsult a healthcare professional for further advice.\n<Image Context Information>: {image_context_information}\n<Question>: Please provide a description of the given medical image.\nPlease respond to the <Question> as instructed.\nFigure19: PromptforLLM-Reformatted. {image_context_information} pertainstoimagecaptions\nandinlinementions.\n18"},"Question":"What is observed directly in the given medical image?","Tool":"similarity_search"},{"index":31,"Answer":{"source":"Papers\/From Text to Multimodality_ Exploring the Evolution and Impact of Large Language Models in Medical Practice.pdf (page 7)","content":"stakes medical decisions. Clinicians need to understand the markprovidesagoodexampleofacomprehensivedatasetthat\nrationale behind AI-generated outputs to trust and validate addresses some of these challenges [48].\ntheir recommendations. The lack of transparency and inter- Privacy-PreservingDataSharing:Investigatinginnovative\npretability in current LLMs hinders the ability to identify methods like federated learning [33] to enable collaborative\npotential biases, errors, or inconsistencies in their reasoning. data sharing and model training while preserving patient\nFurther research to understand how LLMs make decisions, privacy.\nparticularlyinthecontextofassessingclinicalacuityisneeded Standardization and Interoperability: Developing stan-\n[31].DevelopingmethodstomakeLLMsmoretransparentand dardizeddata formatsand categoriesto facilitate data integra-\ninterpretable is crucial for ensuring their safe and responsible tion and interoperability across different healthcare systems\nuse in medical applications [104], [105]. and institutions. This is crucial for training models that can\nEnhancing Generalization and Robustness generalize well to new settings [111].\nAchieving reliable generalization and robustness across di-\nB. Advanced Modality Alignment\nverse medical contexts, patient populations, and languages is\ncrucial for the real-world deployment of MLLMs. Current A key area for future research is developing more so-\nmodelsoftenstruggletogeneralizebeyondtheirtrainingdata, phisticated methods for aligning different modalities. Future\nwhich leads to inaccuracies and biases when applied to new research could focus on developing novel architectures and\npopulations or scenarios. The study by Zhang et al. demon- trainingstrategiesthatcanbettercapturethecomplexrelation-\nstrates that while LLMs can effectively analyze data from ships between different modalities, leading to more accurate\nspecificmedicalspecialties,theirperformanceoftendecreases and robust predictions [112]."},"Question":"What is crucial for ensuring the safe and responsible use of LLMs in medical applications?","Tool":"similarity_search"},{"index":32,"Answer":{"source":"Papers\/From Text to Multimodality_ Exploring the Evolution and Impact of Large Language Models in Medical Practice.pdf (page 1)","content":"training phase allows them to learn a wide range of linguistic\nradiology tasks. This model has surpassed larger counterparts\nfeatures and relationships, making them adaptable to various\nin both performance and accessibility, making it particularly\ndownstream tasks.\nsuitable for real-world clinical applications [6].\nLLMs can be fine-tuned on smaller, task-specific datasets\nMLLMs could also enhance communication between pa-\nto further refine their performance in specific domains. For\ntientsandhealthcareprovidersthroughinteractivechatbotsand\nexample,ClinicalT5[37]demonstrateshowageneral-purpose\nvirtual assistants, potentially improving patient engagement\nLLM(T5)canbe adaptedforclinicaltextbyfine-tuningiton\nand healthcare accessibility [8]. The creation of chatbots\ntheMIMIC-IIIdataset.Thisadaptationto themedicaldomain\nlike MedAide, which utilize optimized tiny-LLMs on edge\nis crucial for addressing the unique challenges of medical\ndevices, demonstrates the capacity of MLLMs to provide\nlanguage, including its specialized vocabulary and complex\nmedical assistance in resource-limited settings and remote\nsemantic relationships [38].\nareas,addressingchallengesinhealthcareaccess[9].However,\ndeveloping reliable and trustworthy medical chatbots requires Despiteimpressivecapabilities,LLMsmayfacemanylimi-\naddressingcriticalissues suchas accuracy,privacyprotection, tations. One notableissue is ”hallucination”,wherethe model\nand bias mitigation to meet the high standards required for generates plausible but incorrect or nonsensical information,\npatient care and safety. as highlighted in the study by Ziaei and Schmidgall [39].\nOur review aims to offer an overview of the current state Hallucination can be particularly problematic in healthcare,\nof MLLMs in medicine and healthcare. We will not only whereaccuracyandreliabilityarethe toppriorities[28], [40].\nexamine their architecture, capabilities, and limitations, but Additionally,biases presentin the trainingdata can propagate"},"Question":"What is a notable issue that large language models may face in medical applications?","Tool":"similarity_search"},{"index":33,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 20)","content":"whilepreservingtheoriginalparametersofthemodel[212,213].Anotherapproachinvolvesidentifyingparameters\nrelatedtospecificknowledgeandupdatingthemaccordinglytointegraterelevantnewinformation[214,215,216].In\nadditiontomodelediting,retrieval-augmentedgenerationcanbeemployedtoupdatetheknowledgeofmedicalLLMs\nandMLLMsbylinkingthemodeltoaninformationretrievalcomponent.Thisallowsthemodeltoretrieverelevant\ncontentfromexternalknowledgebasesasreferences[16,110],therebygeneratingmorereliableresponses.\n6.4. PrivacyandSecurity\nMedicalLLMsandMLLMsaretrainedonalarge-scalemedicalcorpusthatincludesdatasuchasEHRs,doctor-\npatient dialogues, and other information that may involve patient privacy, including names, phone numbers, and\nemailaddresses.ThisinformationcanpotentiallybedirectlyextractedfrommedicalLLMsorMLLMsusingspecific\npromptingmethods[217,218],raisingsignificantprivacyandsecurityconcerns.\nCurrently,acommonpracticetoenhancepatientprivacyprotectionisdatade-identification[16,23].Thisprocess\ninvolvesremovingoranonymizingsensitiveinformationfromdatasets,includingnames,phonenumbers,addresses,\nand medical record numbers. Additionally, sensitive terms in clinician-patient dialogues and medical records are\nreplaced or removed to dissociate these terms from specific patients. Moreover, differential privacy methods can\neffectively mitigate the risk of privacy breaches by adding noise to obscure individual information in the training\ndata,thuspreventingtheinferenceofspecificdetailswhilestillenablingmeaningfuldataanalysis[219].Furthermore,\nutilizinghigh-qualitysyntheticdatageneratedbymodelssuchasChatGPTorGPT-4duringtrainingensuresboththe\ncontrollabilityanddiversityofthedatasetswhilemitigatingtheriskofprivacyleaks.Itisalsoadvisabletomonitorand\nfilterthemodel’soutputs.Forinstance,iftheoutputcontainssensitivedatasuchasnamesorcontactinformation,this\ninformationshouldberemovedormodifiedduringpost-processing.Finally,weurgedeveloperstoadheretoethical"},"Question":"What is an approach to updating knowledge in medical language models?","Tool":"similarity_search"},{"index":34,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 5)","content":"BioMedLM[81] Transformer 2.7 PubMedCenter,Pile PT,SFT AEM 2024\/03\nPediatricsGPT[82] Baichuan2 7\/13 PedCorpus CPT,SFT,DPO AEM,Human,AI 2024\/05\nDoctorGLM[83] ChatGLM 6 ChatDoctor,HealthcareMagic,MedDialog,CMD. IFT Human 2023\/04\nEncoder-Decoder BianQue[84] ChatGLM 6 BianQueCorpus IFT AEM 2023\/10\nSoulChat[22] ChatGLM 6 SoulChatCorpus IFT AEM,Human 2023\/11\n1Encoder-onlymodelsarenotincludedastheytypicallybelongtothePLM,notLLM.\n2\"CPT\"meanscontinuouspre-training,\"IFT\"meansinstructionfine-tuning,\"SFT\"meanssupervisedfine-tuning,\"RLHF\"meansreinforcementlearningfromhumanfeedback,\"RLAIF\"meansreinforcement\nlearningfromAIfeedback,\"DPO\"meansdirectpreferenceoptimization.\n3\"AEM\"meansautomaticevaluationmetrics.\nutilizethemaskedlanguagemodeling(MLM)taskduringpre-training,whererandomtokensinsentencesaremasked,\nand the model is trained to predict these tokens accurately. This pre-training approach equips encoder-only LMs\nwith exceptional natural language understanding capabilities, allowing them to effectively encode and comprehend\nmedicalknowledge,therebyimprovingperformanceinvariousmedicaltasks.Consequently,researchershavefocused\non developing dedicated encoder-only LMs tailored specifically for the medical domain [46, 88, 45]. For example,\nBioBERT[46],pre-trainedonbiomedicalcorpora,achievedstate-of-the-artresultsintaskssuchasbiomedicalnamed\nentityrecognition,relationextraction,andQA.MentalBERT[88]wastrainedondatasetsofmentalhealthdisorders\n(e.g., depression, anxiety, suicidal ideation) sourced from social platforms like Reddit and Twitter, facilitating its\napplicationinmentalhealthresearch.\nDespitethepresenceofnumerousencoder-onlyLMsinthemedicaldomain,thesemodelsarebetterclassifiedas\npre-trainedlanguagemodels(PLMs)[33,34]ratherthanLLMs.Thisdistinctionarisesbecausetheyrequirefine-tuning\nfordownstreamtasks,lackingtherobustICLandfew-shotcapabilitiesofmodelslikeGPT-3.Therefore,thesePLMs\nwillnotbefurtheraddressedinthefollowingsections.\n3.1.2. Decoder-only"},"Question":"\"What is the purpose of equipping encoder-only LMs with exceptional natural language understanding capabilities?\"","Tool":"similarity_search"},{"index":35,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 9)","content":"nimedvqa: Anewlarge-scalecomprehensiveevaluationbenchmarkformedicallvlm. arXiv\npreprintarXiv:2402.09181,2024. 1,3,6,9\n[11] PengXia,ZeChen,JuanxiTian,YangruiGong,RuiboHou,YueXu,ZhenbangWu,Zhiyuan\nFan,YiyangZhou,KangyuZhu,etal. Cares: Acomprehensivebenchmarkoftrustworthiness\ninmedicalvisionlanguagemodels. arXivpreprintarXiv:2406.06007,2024. 1,3\n[12] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.Advances\ninneuralinformationprocessingsystems,36,2024. 1,3,5,8,9\n[13] GuimingHardyChen,ShunianChen,RuifeiZhang,JunyingChen,XiangboWu,ZhiyiZhang,\nZhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-\nsynthesizeddataforalitevision-languagemodel. arXivpreprintarXiv:2402.11684,2024. 1,3,\n5,9\n[14] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi. Blip-2: Bootstrappinglanguage-image\npre-trainingwithfrozenimageencodersandlargelanguagemodels. InInternationalconference\nonmachinelearning,pages19730–19742.PMLR,2023. 3,9\n[15] Run-ZeFan,XuefengLi,HaoyangZou,JunlongLi,ShwaiHe,EthanChern,JiewenHu,and\nPengfeiLiu. Reformattedalignment. arXivpreprintarXiv:2402.12219,2024. 3\n[16] LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahua\nLin. Sharegpt4v: Improvinglargemulti-modalmodelswithbettercaptions. arXivpreprint\narXiv:2311.12793,2023. 3,5\n10"},"Question":"What is the main purpose of a comprehensive evaluation benchmark formed in medical vision language models?","Tool":"similarity_search"},{"index":36,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 8)","content":"PromptAugmentation V Chis au ta Cl AM De +d-A [1l 2p 1ac ]a[92] E Ex xp pe er rt tm mo odd ee ll ss M Che ad t- GA Plp Taca CR hO eC XO p, erB t,ig MB II MO IC-CXR \/ AEM 2 20 02 23 3\/ \/0 04 5\nOphGLM[122] Expertmodels ChatGLM Webdata,MedDialog AEM 2023\/06\nGATED XATTN-DENSE Layers, introduced in Flamingo [54], incorporate dense cross-attention layers into a\nfrozenpre-trainedLLM.Thesecross-attentionlayersreceiveinformationfromthevisionencoder’soutput,whichis\ntypicallyprocessedthroughaPerceiverResampler[123]toreducethecomputationalcomplexityofvision-textcross-\nattention.Usingadditionalcross-attentionlayers,theLLMgeneratestextresponsesbasedonvisualrepresentations.\nSubsequentworkssuchasMed-Flamingo[102],whicharebasedonFlamingo,alsoutilizethesecross-attentionlayers\nformodalityalignment.\nQuery-Basedmethod,oftenconsideredamultimodalperceiver[124],extractsinformationfromvisualrepresen-\ntationsthroughasetoflearnablequeryvectors.Forexample,theQ-FormerintroducedinBLIP-2[125]extractsvisual\nfeaturesfromafrozenvisionencoder,enablingLLMstogeneratetextresponsesalignedwithvisualinformation.This\nquery-basedapproachcanbeeffectivelyextendedto3Dspaces,asdemonstratedbyChenetal.[103]withMedBLIP,\nwhich adapts the querying mechanism for 3D medical imaging. Although these methods can represent images with\nonlyafewqueries,thusreducingtrainingcosts,theyrisklosingcriticalvisualinformation.Moreover,Yaoetal.[126]\ndemonstratedthattheQ-Formerfunctionsmerelyasaninefficientvisualtokencompressor.Forcost-effectivetoken\nreduction,adaptiveaveragepoolingoutperformsit.\nProjection-Basedmethodcanberegardedasatypeofmultimodalconverter[124].Itissimplerthanthequery-\nbasedmethod,asitmapsvisualrepresentationsfromthevisionencoder’soutputtothewordembeddingspaceusinga\nsimpleprojectionlayer,allowingLLMstointerpretimages.Forexample,LLaVA-Med,Qilin-Med-VL,andXrayGPT\n[111] employ a simple linear layer to map visual representations, and MedVIntTE [109] and LLaVA-1.5 [127] rely"},"Question":"\"What do cross-attention layers in frozen-pre-trained LLMs receive information from?\"","Tool":"similarity_search"},{"index":37,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 20)","content":"standards,ensuringthatmodelsrespectprivacyrightswhenhandlingpatientdataandprovidepatientswiththeright\ntoaccessanddeletetheirinformation.\n6.5. BiasandToxicity\nLarge-scale corpora, particularly data sourced from the internet, inevitably contain various biased viewpoints,\nwhichLLMsandMLLMsmaylearn[26,220],includingbiasesrelatedtorace[221],gender[222],andpolitics[223].\nAdditionally, language models may generate toxic responses, such as aggressive and hurtful remarks, with certain\ngroups being more likely to be targeted due to these biases [161]. These biases and toxicities extend to LLMs and\nMLLMs,posingpotentialimplicationsandthreatstopatients,andmayhaveseriousconsequencesforindividualswith\nmentalillness.\nReducingbiasintrainingdataisafundamentalapproachtomitigatingbiasinmodels.Specifically,carefulcuration\nand screening of diverse, balanced, and representative training data ensure that models learn from a broader range\nXiao et al.: PreprintsubmittedtoElsevier Page 21 of 32"},"Question":"What is the main approach to mitigating bias in models?","Tool":"similarity_search"},{"index":38,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 15)","content":"an AI model’s visual capabilities in handling complex medical images. Team\nmembers should inquire about subtle details in the image.\nFigure14: PromptforEvaluatorandAIModelScenario: Asimulatedinteractionwhereaquality\ncontrolteammemberassessesanAImodel’sabilitytoanalyzecomplexmedicalimages.\n16"},"Question":"What subtle details in an image should a team member inquire about?","Tool":"similarity_search"},{"index":39,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 24)","content":"[41] FranzJosefOch,DanielGildea,SanjeevKhudanpur,AnoopSarkar,KenjiYamada,AlexanderFraser,ShankarKumar,LibinShen,DavidA\nSmith,KatherineEng,etal.Asmorgasbordoffeaturesforstatisticalmachinetranslation.InProceedingsoftheHumanLanguageTechnology\nConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HLT-NAACL2004,pages161–168,2004.\n[42] YannLeCun,YoshuaBengio,andGeoffreyHinton.Deeplearning.Nature,521(7553):436–444,2015.\n[43] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal.Improvinglanguageunderstandingbygenerativepre-training.2018.\n[44] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert:Pre-trainingofdeepbidirectionaltransformersforlanguage\nunderstanding.arXivpreprintarXiv:1810.04805,2018.\n[45] YuGu,RobertTinn,HaoCheng,MichaelLucas,NaotoUsuyama,XiaodongLiu,TristanNaumann,JianfengGao,andHoifungPoon.\nDomain-specificlanguagemodelpretrainingforbiomedicalnaturallanguageprocessing. ACMTransactionsonComputingforHealthcare\n(HEALTH),3(1):1–23,2021.\n[46] JinhyukLee,WonjinYoon,SungdongKim,DonghyeonKim,SunkyuKim,ChanHoSo,andJaewooKang.Biobert:apre-trainedbiomedical\nlanguagerepresentationmodelforbiomedicaltextmining.Bioinformatics,36(4):1234–1240,2020.\n[47] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Languagemodelsareunsupervisedmultitask\nlearners.OpenAIblog,1(8):9,2019.\n[48] BertalanMeskó.Promptengineeringasanimportantemergingskillformedicalprofessionals:tutorial.JournalofMedicalInternetResearch,\n25:e50638,2023.\n[49] AohanZeng,XiaoLiu,ZhengxiaoDu,ZihanWang,HanyuLai,MingDing,ZhuoyiYang,YifanXu,WendiZheng,XiaoXia,etal.Glm-130b:\nAnopenbilingualpre-trainedmodel.arXivpreprintarXiv:2210.02414,2022.\n[50] RowanZellers,XimingLu,JackHessel,YoungjaeYu,JaeSungPark,JizeCao,AliFarhadi,andYejinChoi. Merlot:Multimodalneural\nscriptknowledgemodels.AdvancesinNeuralInformationProcessingSystems,34:23634–23651,2021.\n[51] LisaAnneHendricks,JohnMellor,RosaliaSchneider,Jean-BaptisteAlayrac,andAidaNematzadeh.Decouplingtheroleofdata,attention,"},"Question":"\"What is the role of data, attention, and models being decoupled in a particular context?\"","Tool":"similarity_search"},{"index":40,"Answer":{"source":"Papers\/From Text to Multimodality_ Exploring the Evolution and Impact of Large Language Models in Medical Practice.pdf (page 10)","content":"age-related diseases,” TheJournals ofGerontology: Series A,vol.79, 2024.\nno.9,2024. [99] T.GroteandP.Berens,“Aparadigmshift?—Ontheethicsofmedical\n[79] Ayo Adedeji, Sarita Joshi, and Brendan Doohan, “The Sound of largelanguagemodels,”Bioethics,vol.38,no.5,pp.383–390,mar25\nHealthcare: Improving Medical Transcription ASR Accuracy with 2024.\nLargeLanguageModels,”arXiv.org,2024. [100] H. Huang, O. Zheng, D. Wang, J. Yin, Z. Wang, S. Ding, H. Yin,\n[80] Y. Artsi, V. Sorin, E. Konen, B. S. Glicksberg, G. Nadkarni, and C. Xu, R. Yang, Q. Zheng, and B. Shi, “Chatgpt for shaping the\nE.Klang,“Largelanguagemodelsinsimplifyingradiological reports: futureofdentistry:thepotentialofmulti-modallargelanguagemodel,”\nsystematic review,” jan92024. International JournalofOralScience, vol.15,no.1,jul282023.\n[81] B.Boecking,N.Usuyama,S.Bannur,D.C.Castro,A.Schwaighofer, [101] M.Hu,J.Qian,S.Pan,Y.Li,R.L.J.Qiu,andX.Yang,“Advancing\nS. Hyland, M. Wetscherek, T. Naumann, A. Nori, J. Alvarez-Valle, medicalimagingwithlanguagemodels:featuringaspotlightonChat-\nH. Poon, and O. Oktay, Making the Most of Text Semantics to GPT,”PhysicsinMedicine&Biology,vol.69,no.10,p.10TR01,may\nImprove Biomedical Vision–Language Processing. Springer Nature 32024.\nSwitzerland, 2022,pp.1–21. [102] A. Youssef, S. Stein, J. Clapp, and D. Magnus, “The Importance of\n[82] Sheng Zhang, Yanbo Xu, Naoto Usuyama, J. Bagga, Robert Tinn, Understanding Language in Large Language Models,” The American\nSam Preston, Rajesh N. Rao, Mu-Hsin Wei, Naveen Valluri, Cliff JournalofBioethics, vol.23,no.10,pp.6–7,oct32023.\nWong,M.Lungren,TristanNaumann,andHoifungPoon,“Biomedclip: [103] Jun-En Ding, Phan Nguyen Minh Thao, Wen-Chih Peng, Jian-Zhe\na multimodal biomedical foundation model pretrained from fifteen Wang, Chun-Cheng Chug, Min-Chen Hsieh, Yun-Chien Tseng, Ling\nmillion scientific image-text pairs,”2023. Chen,DongshengLuo,Chi-TeWang,Pei-fuChen,FengLiu,andFang-"},"Question":"What is the significance of \"paradigm shift\" in the context of medical ethics?","Tool":"similarity_search"},{"index":41,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 16)","content":"InternandSpecialistDoctor\nYou need to generate a question-and-answer pair based on this image. You\nshould adopt the tone of an intern to ask questions and a specialist doctor\nto answer them. The answers should provide useful information derived from\nthe image and give a detailed analysis. The response should provide detailed\nand rich useful information.\nFigure15: PromptforInternandSpecialistDoctorScenario: Asimulateddialoguewhereanintern\nasksquestionsandaspecialistprovidesdetailed,informativeanswersbasedonamedicalimage.\nMedicalTeacherandStudent\nYou need to generate a question-and-answer pair based on this image. You\nneed to act as a medical teacher and a student, engaging in an educational\ninteraction about the image. The teacher should pose questions, asking the\nstudent to analyze the image and propose possible diagnoses. The student\nshould answer the questions and explain their observations and reasoning\nprocess.\nFigure16: PromptforMedicalTeacherandStudentScenario: Asimulatededucationalinteraction\nwheretheteacherpromptsthestudenttoanalyzeamedicalimageandproposepotentialdiagnoses.\nSeniorDoctorandIntern\nYou need to generate a question-and-answer pair based on this image.\nYou should act as a senior doctor and an intern, discussing the image.\nThe senior doctor should pose relevant questions to test the intern’s\nobservational and analytical skills concerning the image, while the intern\nshould respond and explain their viewpoint.\nFigure17: PromptforSeniorDoctorandInternScenario: Asimulateddialoguewhereasenior\ndoctor tests an intern’s observational and analytical skills through questions based on a medical\nimage.\nE PromptsforEvaluation\nDuringtheevaluation,weusedaunifiedtemplate.\nPromptforEvaluation\n<question>\nA. <option_1>\nB. <option_2>\nC. <option_3>\nD. <option_4>\nAnswer with the option’s letter from the given choices directly.\nFigure18: PromptforEvaluation.\n17"},"Question":"A. What is the role of an InternandSpecialistDoctor in this context?","Tool":"similarity_search"},{"index":42,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 30)","content":"4026,2023.\n[201] ZhuofanZong,BingqiMa,DazhongShen,GuangluSong,HaoShao,DongzhiJiang,HongshengLi,andYuLiu.MoVA:Adaptingmixture\nofvisionexpertstomultimodalcontext.InTheThirty-eighthAnnualConferenceonNeuralInformationProcessingSystems,2024.\n[202] XiangLisaLiandPercyLiang.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.InProceedingsofthe59thAnnualMeetingof\ntheAssociationforComputationalLinguisticsandthe11thInternationalJointConferenceonNaturalLanguageProcessing,pages4582–\n4597.AssociationforComputationalLinguistics,August2021.\n[203] EdwardJHu,yelongshen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,andWeizhuChen. LoRA:Low-rank\nadaptationoflargelanguagemodels.InInternationalConferenceonLearningRepresentations,2022.\n[204] NeilHoulsby,AndreiGiurgiu,StanislawJastrzebski,BrunaMorrone,QuentinDeLaroussilhe,AndreaGesmundo,MonaAttariyan,and\nSylvainGelly. Parameter-efficienttransferlearningfornlp. InInternationalConferenceonMachineLearning,pages2790–2799.PMLR,\n2019.\n[205] XiangxiangChu,LimengQiao,XinyangLin,ShuangXu,YangYang,YimingHu,FeiWei,XinyuZhang,BoZhang,XiaolinWei,etal.\nMobilevlm:Afast,reproducibleandstrongvisionlanguageassistantformobiledevices.arXivpreprintarXiv:2312.16886,2023.\n[206] ZhengqingYuan,ZhaoxuLi,WeiranHuang,YanfangYe,andLichaoSun.TinyGPT-v:Efficientmultimodallargelanguagemodelviasmall\nbackbones. In2ndWorkshoponAdvancingNeuralNetworkTraining:ComputationalEfficiency,Scalability,andResourceOptimization,\n2024.\n[207] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella,\nKranthiKiranGV,etal. RWKV:ReinventingRNNsforthetransformerera. InFindingsoftheAssociationforComputationalLinguistics:\nEMNLP2023,pages14048–14077.AssociationforComputationalLinguistics,December2023.\n[208] AlbertGuandTriDao. Mamba:Linear-timesequencemodelingwithselectivestatespaces. InFirstConferenceonLanguageModeling,\n2024.\nXiao et al.: PreprintsubmittedtoElsevier Page 31 of 32"},"Question":"Here's an example of a question that can only be answered based on this context:\n\nWhat type of adaptation is discussed in [201]?","Tool":"similarity_search"},{"index":43,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 3)","content":"A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine\nFeature Structure Objective Prompt Data\nEngineering Engineering Engineering Engineering Engineering\nUnsupervised Pre-training Unsupervised Pre-training\nSupervised Learning Text-only to Multimodal High-quality Data\n& Fine-tuning & Prompt\nGPT-1 Bert GPT-2 GPT-3 Flamingo LIMA\nBeginning of LLMs Beginning of MLLMs\nFig. 3. Evolution of LLMs and MLLMs. The Evolution of LLMs and MLLMs. The upper section illustrates the research\nfocusesandparadigmshiftsacrosstheevolutionofthesemodels,whilethelowersectionhighlightskeymilestonesachieved\nat each stage.\ntothescarcityofspecializedannotatorsandthecomplexityoftheannotationprocess[24].TheintroductionofTrans-\nformer[1]revolutionizedtheNLPlearningparadigm,renderingsupervisedlearningincreasinglymarginalized[39].\nTransformer-based models like GPT [43] and BERT [44] achieved state-of-the-art results through unsupervised\npre-training on large-scale unlabeled text, followed by supervised fine-tuning (SFT) using task-specific objective\nfunctions. The emergence of GPT and BERT introduced a new NLP paradigm: unsupervised pre-training & fine-\ntuning fine-tuning. This paradigm also revolutionized language model development in the medical field, giving rise\ntoprominentmodelslikePubMedBERT[45]andBioBERT[46].Comparedtoearliermodels,theseapproachesoffer\nseveraladvantages:(1)Pre-trainingdatacanbedrawnfromanyunannotatedtextcorpus,suchasbiomedicalliterature,\nmitigatingchallengesrelatedtolimitedannotateddatainmedicaldomains;(2)Trainingonlarge-scaleunlabeleddata\nenablestheacquisitionofgeneralandabstractlanguagerepresentations,improvinggeneralization;(3)Fine-tuningfor\ndownstreamtasksrequiresonlytask-specificobjectivefunctions,eliminatingextensivearchitecturalmodificationsand\nfacilitatingatransitionfromstructuraltoobjectiveengineering.\n2.3. UnsupervisedPre-trainingandPrompt\nWhileGPTandBERTachievedstate-of-the-artresultsintaskslikemachinetranslation,sentimentanalysis,and"},"Question":"What is the main purpose of unsupervised pre-training and fine-tuning in language model development?","Tool":"similarity_search"},{"index":44,"Answer":{"source":"Papers\/From Text to Multimodality_ Exploring the Evolution and Impact of Large Language Models in Medical Practice.pdf (page 0)","content":"4202\nceD\n01\n]YC.sc[\n5v21810.0142:viXra\nFrom Text to Multimodality: Exploring the\nEvolution and Impact of Large Language Models in\nMedical Practice\nQian Niu1, Keyu Chen2, Ming Li2, Pohsun Feng3, Ziqian Bi4, Lawrence KQ Yan5, Yichao Zhang6,\nCaitlyn Heqi Yin7, Cheng Fei8, Junyu Liu1, Tianyang Wang9, Yunze Wang10, Silin Chen11, Ming Liu12, Benji Peng*,2\n1Kyoto University\n2Georgia Institute of Technology\n3National Taiwan Normal University\n4Indiana University\n5Hong Kong University of Science and Technology\n6The University of Texas at Dallas\n7University of Wisconsin-Madison\n8Cornell University\n9University of Liverpool\n10University of Edinburgh\n11Zhejiang University\n11Purdue University\n*Corresponding Email: benji@appcubic.com\nIndex Terms—large language models, medical practice, multi- and a challenge: ingesting this information can revolutionize\nmodality, artificial intelligence healthcare, but doing so requires innovative tools capable\nAbstract—Large Language Models (LLMs) have rapidly of processing and synthesizing these diverse data streams.\nevolved from text-based systems to multimodal platforms, sig- Artificialintelligence(AI)hasemergedasa powerfulforcein\nnificantly impacting various sectors including healthcare. This\naddressingthischallenge,withlargelanguagemodels(LLMs)\ncomprehensive review explores the progression of LLMs to\nat the forefront of this revolution.\nMultimodalLargeLanguageModels(MLLMs)andtheirgrowing\ninfluenceinmedicalpractice.Weexaminethecurrentlandscape Initially, LLMs focused primarily on text-based tasks,\nof MLLMs in healthcare, analyzing their applications across demonstrating remarkable proficiency in understanding and\nclinical decision support, medical imaging, patient engagement, generating human-like language [2]. However, the inherent\nand research. The review highlights the unique capabilities\nmultimodalityofmedicine,whereclinicaldecisionsoftenrely\nof MLLMs in integrating diverse data types, such as text,\non the synthesis of information from diverse sources such"},"Question":"What is the main purpose of exploring the evolution and impact of Large Language Models in medical practice?","Tool":"similarity_search"},{"index":45,"Answer":{"source":"Papers\/From Text to Multimodality_ Exploring the Evolution and Impact of Large Language Models in Medical Practice.pdf (page 3)","content":"information. Aligning these modalities is essential for LLMs A. A. Clinical Decision Support\nto process and reason over multimodal data. Researchers are While MLLMs integrate diverse data modalities, offering\ncurrently exploring several methods for addressing this issue, a more comprehensive view of patient health and the ability\nwhich can be grouped into four main categories. to detect complex patterns for improved diagnosis, treatment\n• Multimodal Converters: These methods transform data personalization, and risk assessment [58], their development\nfromdifferentmodalitiesintoaunifiedrepresentationthat is still in the early stages. As a result, LLMs continue\nLLMs can understand. For example, images might be to dominate the field due to their maturity and established\nconvertedinto textual descriptions or embeddingsbefore performance.ThissectionintroducesbothLLMsandMLLMs,\nbeingfedintothe LLM.Thisapproachisseen inmodels while emphasizing the promise of multimodal models.\nlike X-LLM [19], which treats modalities as foreignlan- Diagnosis and Treatment Recommendations: NYUTron\nguagesandconvertsthemtotext,orLIFTED[56],which is an LLM trained on clinical notes, for predicting patient\ntransforms modalities into natural language descriptions outcomes with high accuracy [1]. PMC-LLaMA is a perfor-\nfor improved clinical trial outcome prediction. mant LLM for medical Q&A [59]–[61]. Almanac is an LLM\n• MultimodalPerceivers:Thesemethodsdirectlyenhance augmented with retrieval capabilities from curated medical\nthe LLMs’ perception of multimodal data. A vision resourcesandhassignificantimprovementsinfactuality,com-\nencoder can be integrated into the LLM architecture to pleteness, user preference, and safety for clinical decision-\nenable it to directly process and understand images and making[12].Med-PaLM2isaspecializedLLMformedicine,"},"Question":"What are four main categories of methods for addressing an issue in multimodal data processing?","Tool":"similarity_search"},{"index":46,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 11)","content":"psychologicalcounselingaudiorecordingsintotextandutilizedGPT-4toextractquestion-answerpairsandgenerate\nkeysummaries,providingsupplementarycontextualinformationforconstructingthePsych8kdataset.\n4.2. Fine-TuningMethods\nThe extensive parameters in LLMs and MLLMs make training medical LLMs and MLLMs from scratch\ncomputationallyintensive.Consequently,theprevalentmethodforconstructingmedicalLLMsandMLLMsinvolves\nfine-tuning general foundation models using medical datasets. This section outlines six fine-tuning methods, as\nillustrated in Fig. 5, to aid researchers in developing medical LLMs and MLLMs. In addition, this section explores\nthecharacteristicsofthesefine-tuningmethods,providingpracticalguidanceforselectingtheappropriatefine-tuning\nstrategies.\nFine-Tuning Methods for LLMs and MLLMs\nCPT IFT\nLiteratures Medical Instruction-\nKnowledge Following\nInstruction-\nMedical Books\nFollowing Data\nClinical\nGuidelines\nRLHF \/RLAIF \/DPO SFT\nHuman Downstream QA\nPerference Tasks\nPreference\nDialogue\nData\nRadiology\nReports\nFig. 5. Overview of six fine-tuning methods. In our analysis of the related work on medical LLMs and MLLMs, we found\nthat Continuous Pre-Training (CPT) is commonly used to inject medical knowledge into LLMs and MLLMs; Instruction\nFine-Tuning (IFT) enhances the models’ ability to follow instructions and their zero-shot performance; Supervised Fine-\nTuning is frequently employed to improve model performance on specific tasks; and Reinforcement Learning from Human\nFeedback (RLHF), Reinforcement Learning from AI Feedback (RLAIF), and Direct Preference Optimization (DPO) are\nused to align model behavior with human preferences.\nXiao et al.: PreprintsubmittedtoElsevier Page 12 of 32"},"Question":"Here is a possible question that can only be answered based on this context:\n\nWhat are six methods used to fine-tune language models in the field of medicine?","Tool":"similarity_search"},{"index":47,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 5)","content":"• LLaVA-v1.5-LLaMA3-8B + LLaVA_Med This model uses both LLaVA-1.5 data and\nLLaVA_Med’stwo-stagedata. ThedatadistributionisPretraining: 558K(LLaVA)+ 457K\n(LLaVA_MedAlignment);Finetuning: 658K(LLaVA)+57K(LLaVA_MedVQA).\n• LLaVA-v1.5-LLaMA3-8B+ PubMedVision ThismodelusesbothLLaVA-1.5dataand\nPubMedVisiondata. ThedatadistributionisPretraining: 558K(LLaVA)+647K(PubMed-\nVisionAlignmentVQA);Finetuning: 658K(LLaVA)+647K(PubMedVisionInstruction-\nTuningVQA).\nHuatuoGPT-Vision BuildingonPubMedVision,wedevelopedourspecializedmedicalMLLM,\nHuatuoGPT-Vision.ItenhancesLLaVA-v1.5-LLaMA3-8B+ PubMedVision byfeaturing:(1)alarger\nmodel,utilizingYi-1.5-34B[20]asthefoundationalLLM;(2)bilingualcapabilities,supportedbyan\nadditional348KChinesemedicalVQAdatasettranslatedfromPubMedVision;and(3)enhanced\nmedicalknowledge,withaddedtrainingfromthemedicaltextcorpusofHuatuoGPT-II[21].\nBaselines Wecomparedtwotypesofopen-sourcemodels: (1)MedicalMLLMs. Weevaluated\nthreeMedicalMLLMs, includingMed-Flamingo[22], RadFM[8], andLLaVA-Med-7B[7]. (2)\nGeneralMLLMs. WecomparedthelatestmodelsintheLLaVAseries, includingLLaVA-v1.6-\n7B, LLaVA-v1.6-13B, and LLaVA-v1.6-34B [23]. Additionally, we included comparisons with\nYi-VL-34B[20]andQwen-VL-Chat[24].\nBenchmarks ToverifythemedicalmultimodalcapabilitiesofMLLMs,weemployedthreetypes\nofbenchmarks: (1)MedicalVQABenchmark,forwhichweusedthetestsetsofVQA-RAD[3],\nSLAKE[4],PathVQA[5],andPMC-VQA[6]toassessmedicalquestion-answeringcapabilities.\nSpecifically, for SLAKE, we evaluated using its English CLOSED segment. (2) Multimodal\nBenchmark: MMMU [25] is a popular multimodal benchmark, and we utilized the Health &\nMedicinetrackofMMMU,whichisrelevanttomedicalmultimodality. (3)TraditionalMedical\nImagingTasks. WeusedtheopenaccesspartoftheOmniMedVQAdataset[10],whichincludes42\ntraditionalmedicalimagingdatasets,allformattedasVQA.Notethatforallbenchmarks,weusethe\nzero-shotmethodandthequestiontemplatesetbyLLaVA,asshowninAppendixE.\n4.2 Experiment1: EffectivenessofPubMedVision"},"Question":"What is the main purpose of combining multiple data sources in a medical language model?","Tool":"similarity_search"},{"index":48,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 24)","content":"andlossesinmultimodaltransformers.TransactionsoftheAssociationforComputationalLinguistics,9:570–585,2021.\nXiao et al.: PreprintsubmittedtoElsevier Page 25 of 32"},"Question":"What is the subject of research discussed in this document?","Tool":"similarity_search"},{"index":49,"Answer":{"source":"Papers\/From Text to Multimodality_ Exploring the Evolution and Impact of Large Language Models in Medical Practice.pdf (page 10)","content":"[88] K. He, R. Mao, Q. Lin, Y. Ruan, X. Lan, M. Feng, and E. Cambria, Clusmann,Marie-ElisabethLessman,S.Foersch,JacquelineLammert,\n“A Survey of Large Language Models for Healthcare: from Data, Maximilian Tschochohei, Dirk Ja¨ger, Manuel Salto-Tellez, Nikolaus\nTechnology, andApplications toAccountability andEthics,”2023. Schultz,DanielTruhn,andJ.N.Kather,“AutonomousArtificialIntel-"},"Question":"Who collaborated on a survey about large language models for healthcare in 2023?","Tool":"similarity_search"},{"index":50,"Answer":{"source":"Papers\/From Text to Multimodality_ Exploring the Evolution and Impact of Large Language Models in Medical Practice.pdf (page 10)","content":"[83] S. Schmidgall, C. Harris, I. Essien, D. Olshvang, T. Rahman, J. W. MingHung,“LargeLanguageMultimodalModelsfor5-YearChronic\nKim,R.Ziaei,J.Eshraghian,P.Abadir,andR.Chellappa,“Addressing DiseaseCohortPrediction UsingEHRData,”arXiv.org,2024.\ncognitive biasinmedical language models,”2024. [104] E.Alsentzer, M.J.Rasmussen,R.Fontoura, A.L.Cull,B.Beaulieu-\n[84] Y.Jin,M.Chandra,G.Verma,Y.Hu,M.DeChoudhury,andS.Kumar, Jones, K. J. Gray, D. W. Bates, and V. P. Kovacheva, “Zero-shot\n“BettertoAskinEnglish:Cross-LingualEvaluationofLargeLanguage Interpretable Phenotyping of Postpartum Hemorrhage Using Large\nModels for Healthcare Queries,” in Proceedings of the ACM Web LanguageModels,”jun12023.\nConference 2024,vol.35. ACM,may132024,pp.2627–2638. [105] AleksaBisercic, MladenNikolic, M.Schaar, BorisDelibasic, P.Lio’,\n[85] P. Qiu, C. Wu, X. Zhang, W. Lin, H. Wang, Y. Zhang, Y. Wang, and A. Petrovic´, “Interpretable Medical Diagnostics with Structured\nand W. Xie, “Towards Building Multilingual Language Model for DataExtraction byLargeLanguageModels,”arXiv.org,2023.\nMedicine,” 2024. [106] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian\n[86] Y. Khare, V. Bagal, M. Mathew, A. Devi, U. D. Priyakumar, and Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng\nC. Jawahar, “Mmbert: Multimodal BERT Pretraining for Improved Gao,“Llava-Med:TrainingaLargeLanguage-and-VisionAssistantfor\nMedical VQA,” in 2021 IEEE 18th International Symposium on Biomedicine in One Day,” Neural Information Processing Systems,\nBiomedical Imaging(ISBI). IEEE,apr132021. 2023.\n[87] N.H.Shah,D.Entwistle, andM.A.Pfeffer, “Creation andAdoption [107] Z.Bi,S.A.Dip,D.Hajialigol,S.Kommu,H.Liu,M.Lu,andX.Wang,\nof Large Language Models in Medicine,” JAMA, vol. 330, no. 9, p. “AiforBiomedicine intheEraofLargeLanguageModels,”2024.\n866,sep52023. [108] Dyke Ferber, O. S. E. Nahhas, Georg Wo¨lflein, Isabella C. Wiest, J."},"Question":"What is the purpose of training large language models for medical applications?","Tool":"similarity_search"},{"index":51,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 0)","content":"simenxiao1211@163.com(HanguangXiao)\nORCID(s):0000-0002-4359-7455(HanguangXiao)\nXiao et al.: PreprintsubmittedtoElsevier Page1of32\n4202\nceD\n03\n]LC.sc[\n3v30680.5042:viXra"},"Question":"Who is being referred to by their ORCID?","Tool":"similarity_search"},{"index":52,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 31)","content":"collaborationstrategyforllmsinmedicaldecisionmaking.arXivpreprintarXiv:2404.15155,2024.\n[228] XiangruTang,AnniZou,ZhuoshengZhang,ZimingLi,YilunZhao,XingyaoZhang,ArmanCohan,andMarkGerstein.MedAgents:Large\nlanguagemodelsascollaboratorsforzero-shotmedicalreasoning.InFindingsoftheAssociationforComputationalLinguistics:ACL2024,\npages599–621.AssociationforComputationalLinguistics,August2024.\n[229] NimeeshaChan,FelixParker,WilliamBennett,TianyiWu,MungYaoJia,JamesFackler,andKimiaGhobadi. Medtsllm:Leveragingllms\nformultimodalmedicaltimeseriesanalysis.arXivpreprintarXiv:2408.07773,2024.\n[230] MinHu,LeiLiu,XiaohuaWang,YimingTang,JiaoyunYang,andNingAn. Parallelmultiscalebridgefusionnetworkforaudio–visual\nautomaticdepressionassessment.IEEETransactionsonComputationalSocialSystems,11(5):6830–6842,2024.\n[231] JianChen,YuzhuHu,QifengLai,WeiWang,JunxinChen,HanLiu,GautamSrivastava,AliKashifBashir,andXipingHu.Iifdd:Intraand\ninter-modalfusionfordepressiondetectionwithmulti-modalinformationfrominternetofmedicalthings.InformationFusion,102:102017,\n2024.\n[232] LangHe,MingyueNiu,PrayagTiwari,PekkaMarttinen,RuiSu,JieweiJiang,ChenguangGuo,HongyuWang,SongtaoDing,Zhongmin\nWang,XiaoyingPan,andWeiDang. Deeplearningfordepressionrecognitionwithaudiovisualcues:Areview. InformationFusion,\n80:56–86,2022.\nXiao et al.: PreprintsubmittedtoElsevier Page 32 of 32"},"Question":"What is the role of large language models in medical decision making?","Tool":"similarity_search"},{"index":53,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 4)","content":"employed in medical MLLMs. For clarity, Table 1 and Table 2 detail and categorize existing medical LLMs and\nMLLMs.\n3.1. StructureofLLMs\n3.1.1. Encoder-only\nEncoder-onlylanguagemodels(LMs)consistofmultipleencoderlayerswithintheTransformerarchitecture,with\nBERT being the earliest and most representative example. Inspired by BERT, additional encoder-only LMs such as\nDeBERTa DeBERTa [85], ALBERT [86], and RoBERTa [87] have been developed. Encoder-only LMs commonly\nXiao et al.: PreprintsubmittedtoElsevier Page 5 of 32"},"Question":"What is an example of a type of language model inspired by BERT?","Tool":"similarity_search"},{"index":54,"Answer":{"source":"Papers\/HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale.pdf (page 5)","content":"Model VQA-RAD SLAKE PathVQA PMC-VQA Avg.\nMed-Flamingo 45.4 43.5 54.7 23.3 41.7\nRadFM 50.6 34.6 38.7 25.9 37.5\nLLaVA-Med-7B 51.4 48.6 56.8 24.7 45.4\nQwen-VL-Chat 47.0 56.0 55.1 36.6 48.9\nYi-VL-34B 53.0 58.9 47.3 39.5 49.7\nLLaVA-v1.6-7B 52.6 57.9 47.9 35.5 48.5\nLLaVA-v1.6-13B 55.8 58.9 51.9 36.6 50.8\nLLaVA-v1.6-34B 58.6 67.3 59.1 44.4 57.4\nOurTraining\nLLaVA-v1.5-LLaMA3-8B 54.2 59.4 54.1 36.4 51.0\n+LLaVA_Med 60.2 61.2 54.5 46.6 55.6\n+ PubMedVision 63.8 74.5 59.9 52.7 62.7\nHuatuoGPT-Vision-34B 68.1 76.9 63.5 58.2 66.7\nTable2: TheresultsofthemedicalVQAbenchmark.\nMedicalVQABenchmarks Table2presentstheresultsofthemedicalVQAbenchmarks. General-\npurposeMLLMs, suchasLLaVA-v1.6, demonstratesuperiorperformancecomparedtomedical-\nspecificMLLMslikeLLaVA-Med-7B,aligningwiththefindingsofpriorstudies[10]. However,the\nadditionofmedicalmultimodaldatatoLLaVA-v1.5-LLaMA3-8Bsignificantlyenhancesperformance,\nrevealingsubstantialpotentialforimprovingmedicalimageunderstanding. Notably,theuseofthe\nPubMedVisionledtoan11.7%increaseinoverallaccuracy,significantlyoutperformingtheearlier\nLLaVA_Meddataset. Additionally,asdetailedinAppendixA,fine-tuningonthetrainingsetsof\nthese four datasets indicates that PubMedVision can also significantly improves performance in\ndownstreammedicalmultimodaltasks.\n6"},"Question":"What is the main difference in performance between medical-specific ML models and general-purpose ML models?","Tool":"similarity_search"},{"index":55,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 0)","content":"A Comprehensive Survey of Large Language Models and\nMultimodal Large Language Models in Medicine\nHanguangXiaoa,∗,1, FeizhongZhoua,1, XingyueLiua, TianqiLiua, ZhipengLia, XinLiua\nand XiaoxuanHuanga\naSchoolofArtificialIntelligence,ChongqingUniversityofTechnology,Chongqing401120,China\nARTICLE INFO ABSTRACT\nKeywords: SincethereleaseofChatGPTandGPT-4,largelanguagemodels(LLMs)andmultimodallarge\nLargelanguagemodel languagemodels(MLLMs)haveattractedwidespreadattentionfortheirexceptionalcapabilities\nMultimodallargelanguagemodel inunderstanding,reasoning,andgeneration,introducingtransformativeparadigmsforintegrat-\nMedicine ing artificial intelligence into medicine. This survey provides a comprehensive overview of\nHealthcare thedevelopment,principles,applicationscenarios,challenges,andfuturedirectionsofLLMs\nClinicalapplication andMLLMsinmedicine.Specifically,itbeginsbyexaminingtheparadigmshift,tracingthe\ntransitionfromtraditionalmodelstoLLMsandMLLMs,andhighlightingtheuniqueadvantages\noftheseLLMsandMLLMsinmedicalapplications.Next,thesurveyreviewsexistingmedical\nLLMs and MLLMs, providing detailed guidance on their construction and evaluation in a\nclearandsystematicmanner.Subsequently,tounderscorethesubstantialvalueofLLMsand\nMLLMsinhealthcare,thesurveyexploresfivepromisingapplicationsinthefield.Finally,the\nsurveyaddressesthechallengesconfrontingmedicalLLMsandMLLMsandproposespractical\nstrategiesandfuturedirectionsfortheirintegrationintomedicine.Insummary,thissurveyoffers\na comprehensive analysis of the technical methodologies and practical clinical applications\nof medical LLMs and MLLMs, with the goal of bridging the gap between these advanced\ntechnologies and clinical practice, thereby fostering the evolution of the next generation of\nintelligenthealthcaresystems.\n1. Introduction\nThe introduction of the Transformer [1] has revolutionized the fields of Natural Language Processing (NLP)\nand Computer Vision (CV). The Transformer’s robust parallel computing capabilities and self-attention mechanism"},"Question":"What is the main purpose of the survey?","Tool":"similarity_search"},{"index":56,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 8)","content":"A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine\nTable 2\nDetailed information on existing medical MLLMs.\nModalityAlignmentMethod ModelName VisionEncoder LLMBackbone DataSource EvaluationMethod Date\nGATEDXATTN-DENSELayers Med-Flamingo[102] CLIP-ViT LLaMA MTB,PMC-OA AEM,Human 2023\/07\nMedBLIP[103] EVA-CLIP-ViT BioMedLM ADNI,NACC,OASIS AEM 2023\/05\nXrayGLM[104] ViT-G ChatGLM MIMIC-CXR,OpenI \/ 2023\/05\nQuery-Based PCLmed[105] EVA-CLIP-ViT ChatGLM ImageCLEF2023captionprediction AEM 2023\/06\nRadFM[106] 3DViT MedLLaMA-13B MedMD,RadMD AEM,Human 2023\/08\nCheXagent[107] EVA-CLIP-ViT Mistral CheXinstruct AEM,Human 2024\/01\nCLIP-ViTw\/GPT2[108] CLIP-ViT GPT2-XL Slake,PathVQA,OVQA AEM 2023\/05\nMedVInt[109] PMC-CLIP-ViT PMC-LLaMA PMC-VQA AEM 2023\/05\nPathAsst[110] PathCLIP-ViT Vicuna PathCap,PathInstruct \/ 2023\/05\nLLaVA-Med[18] CLIP-ViT LLaMA PMC-15M,VQA-RAD,SLAKE,PathVQA AEM,AI 2023\/06\nXrayGPT[111] MedCLIP-ViT Vicuna MIMIC-CXR,OpenI AEM,AI 2023\/06\nMed-PaLMM[21] ViT-e,ViT-22B PaLM MultiMedBench AEM,Human 2023\/07\nR2GenGPT[20] SwinTransformer LLaMA2 IU-Xray,MIMIC-CXR AEM 2023\/09\nProjection-Based Qilin-Med-VL[112] ViT LLaMA-2-Chinese ChiMed-VL \/ 2023\/10\nMAIRA-1[96] RAD-DINO Vicuna MIMIC-CXR AEM,Human 2023\/11\nPeFoM-Med[113] EVA-CLIP-ViT LLaMA2 ROCO,VQA-RAD AEM,Human 2024\/01\nM3D-LaMed[114] 3DViT LLaMA-2 M3D-Data AEM,AI 2024\/03\nMoE-TinyMed[115] CLIP-ViT Phi-2 LLaVA-Med,VQA-RAD,SLAKE,PathVQA AEM 2024\/04\nMAIRA-2[116] Rad-DINO Vicuna MIMIC-CXR,PadChest,USMix AEM 2024\/06\nPathChat[97] UNI LLaMA2 PubMed,WSIs AEM,Human 2024\/06\nHuatuoGPT-Vision[117] CLIP-ViT Yi-1.5 PubMedVision,HuatuoGPT-II AEM 2024\/06\nminiGPT-Med[118] EVA-CLIP-ViT LLaMA2 MIMIC,NLST,SLAKE,RSNA,RadVQA AEM 2024\/07\nSkinGPT-4[119] ViT LLaMA2 SKINCON,Dermnet Human 2024\/07\nLLaVA-Med++[56] CLIP-ViT LLaMA MedTrinity-25M,VQA-RAD,SLAKE,PathVQA AEM 2024\/08\nSigPhi-Med[120] SigLIP Phi-2 LLaVA-Med,VQA-RAD,SLAKE,PathVQA AEM 2024\/10\nChatCAD[17] Expertmodels ChatGPT MIMIC-CXR,CheXpert AEM 2023\/02"},"Question":"What types of modalities are used in the existing medical multimodal language models mentioned in this table?","Tool":"similarity_search"},{"index":57,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 6)","content":"A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine\n3.1.3. Encoder-Decoder\nEncoder-decoder LLMs leverage the Transformer architecture, combining stacks of encoders and decoders. The\nencoder processes input sequence and outputs representations with contextual information, which the decoder uses\nfor text generation. Prominent examples of encoder-decoder LLMs T5 [58] and GLM [91]. Similar to encoder-only\nanddecoder-onlyarchitectures,encoder-decoderLLMshavealsobeenadaptedformedicalapplications.Forexample,\nChen et al. [22] fine-tuned ChatGLM using the empathetic dialogue dataset SoulChatCorpus. The resulting model\nexhibitedrobustempatheticcapabilities,assistingusersinarticulatingtheirthoughtsandofferingsuitablesuggestions\nduringpsychologicalcounseling.\nWhile encoder-decoder LLMs integrate the strengths of encoder-only and decoder-only architectures, balancing\ntext understanding and generation, Wang et al. [89] showed that decoder-only LLMs excel in zero-shot scenarios\nwithout fine-tuning. In contrast, encoder-decoder LLMs require multitask fine-tuning with annotated data to reach\noptimal performance. Since the prevailing LLM training paradigm relies on unsupervised learning on large-scale\ncorpora,decoder-onlyarchitectures,withtheirsuperiorzero-shotperformance,arebettersuitedtoexploitunlabeled\ndata.Asaresult,decoder-onlyarchitecturesremainthepredominantchoiceforLLMs.\n3.2. StructureofMLLMs\nAs shown in Fig. 4, this section provides a detailed discussion of three critical modules in MLLMs: the Vision\nEncoder, the LLM Backbone, and the Modality Alignment Module. The method of leveraging expert models to\nconstructMLLMsistreatedasaformofpromptaugmentationmethod[92]andisdiscussedalongsideothermodality\nalignmentmodules.\nThe heart size and pulmonary vascularity appear within normal limits. A large\nhiatal hernia is noted. The lungs are free of focal airspace disease ... GATED XATTEN-DENSE Layers\nLLM Block\nLarge Language Model Backbone\nGATED XATTEN-\nDENSE"},"Question":"\"What is the purpose of balancing text understanding and generation in a particular architecture?\"","Tool":"similarity_search"},{"index":58,"Answer":{"source":"Papers\/From Text to Multimodality_ Exploring the Evolution and Impact of Large Language Models in Medical Practice.pdf (page 3)","content":"diseases and pandemics. MLLMs could revolutionize various multimodal tasks and often involves developing new\nmedical practices. For instance, in radiology, MLLMs are architectures and training strategies so the modals can\nbeing explored for generating comprehensive reports [54], learn from different modalities simultaneously. Models\nassisting in diagnosis by analyzing both images and clinical like BiomedGPT [23] are trained on those diverse mul-\nnotes [4], and facilitating visual search and querying within timodal datasets. The recent open-source frameworks\npatient imaging history [55]. like Hippocrates [22] further facilitates this approach\nMLLMsformorespecializedmedicaltaskshasalsogained by providing access to training datasets, codebases, and\nmomentum. SkinGPT-4 is a system designed for dermato- evaluation protocols, encouraging further collaborative\nlogical diagnosis using both images and clinical data, which efforts.\noffersautonomousimageevaluationandtreatmentrecommen- Each method has its own strengths and weaknesses. Mul-\ndations[16]. Developingrobustand reliable MLLMs requires timodal converters are relatively simple but may lead to in-\novercoming many challenges. Large, diverse, and unbiased formation loss during conversion [57]. Multimodal perceivers\nmedical datasets across multiple modalities are crucial [48]. canpotentiallycapturericherrelationshipsbetweenmodalities,\nAccuracy, interpretability, explainability, interoperability, and but requires more complex architectures and training. Tools\nethics are important to be discussed before integrating into assistance uses existing knowledge bases and resources but\nexisting clinical workflows [35]. maynotbecomprehensiveorup-to-date.Data-drivenmethods\ncan achieve high performance but require large and diverse\nC. Modality Alignment Methods: Bridging the Semantic Gap\ndatasets.\nIntegrating different data types into LLMs is challenging,\nmainly because of differences in how each type represents III. APPLICATIONS OF MLLMSIN MEDICINE"},"Question":"What is the main challenge in developing robust and reliable Multimodal Language Models?","Tool":"similarity_search"},{"index":59,"Answer":{"source":"Papers\/A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine.pdf (page 20)","content":"A Comprehensive Survey of Large Language Models and Multimodal Large Language Models in Medicine\nandinferencespeed,achievingfivetimestheinferencespeedofTransformerswhileremainingcomparableinscale.\nExtendingthesecomputationallyandinference-efficientmodelarchitecturestomedicalLLMsandMLLMswillhelp\naddressthecurrenttraininganddeploymentchallengesfacedbythesemodels.\n6.3. LackofRecency\nOnce medical LLMs and MLLMs are trained, the knowledge they acquire becomes static. However, as medical\nknowledge is continuously updated, the absence of new concepts will exacerbate the models’ inaccuracies and\nhallucination problems. This is particularly evident when the models encounter new terms that are not present in\nthe training corpus, rendering them unable to comprehend this knowledge [12]. For example, if medical LLMs and\nMLLMsaretrainedexclusivelyondatapriorto2020,theywilllackinformationregardingCOVID-19.Thislimitation\nmaypreventthemodelsfromunderstandingtermssuchas\"COVID-19\"or\"LongCOVID,\"ortheymayincorrectly\nclassify COVID-19 as a known viral variant (e.g., SARS or MERS), leading to the provision of misleading advice.\nConsequently,thelackofrecentknowledgewillsignificantlyhinderthereal-worldapplicationofmedicalLLMsand\nMLLMs.\nTo address the lack of recency resulting from the offline learning of medical LLMs and MLLMs, continual\nparameter updates through fine-tuning methods to synchronize them with current human knowledge is a feasible\nsolution[155].Whilefine-tuningcaninjectnewmedicalconceptsandknowledgeintothemodel,italsointroduces\ntwo challenges: catastrophic forgetting, in which the model forgets previously learned knowledge upon acquiring\nnew information [209]. The second challenge is negative forward transfer, wherein performance on unseen tasks\ndeteriorateswhenlearningnewtasks[210].Toaddresstheseissues,researchershaveproposedmodelediting[211],\noneisintroducingadditionaltrainableparameterstocorrecterroneousresponsesstemmingfromoutdatedknowledge"},"Question":"What is hindering the real-world application of medical LLMs and MLLMs?","Tool":"similarity_search"},{"index":60,"Answer":"The rapid development of multimodal large language models (MLLMs), such as\nGPT-4V, has led to significant advancements. However, these models still face\nchallenges in medical multimodal capabilities due to limitations in the\nquantity and quality of medical vision-text data, stemming from data privacy\nconcerns and high annotation costs. While pioneering approaches utilize\nPubMed's large-scale, de-identified medical image-text pairs to address these\nlimitations, they still fall short due to inherent data noise. To tackle this,\nwe refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in\nan 'unblinded' capacity to denoise and reformat the data, resulting in the\ncreation of the PubMedVision dataset with 1.3 million medical VQA samples. Our\nvalidation demonstrates that: (1) PubMedVision can significantly enhance the\nmedical multimodal capabilities of current MLLMs, showing significant\nimprovement in benchmarks including the MMMU Health & Medicine track; (2)\nmanual checks by medical experts and empirical results validate the superior\ndata quality of our dataset compared to other data construction methods. Using\nPubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows\nsuperior performance in medical multimodal scenarios among open-source MLLMs.","Question":"What's the abstract of the paper huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale?","Tool":"cypher_search"},{"index":61,"Answer":"2405.08603v3","Question":"What's the DOI of the paper a comprehensive survey of large language models and multimodal large language models in medicine?","Tool":"cypher_search"},{"index":62,"Answer":16,"Question":"How many keywords were used?","Tool":"cypher_search"},{"index":63,"Answer":["arxiv","pubmed"],"Question":"Which are the databases available?","Tool":"cypher_search"},{"index":64,"Answer":"2024","Question":"Which year was the paper titled a comprehensive survey of large language models and multimodal large language models in medicine published in?","Tool":"cypher_search"},{"index":65,"Answer":"arxiv","Question":"Which database indexed the paper a comprehensive survey of large language models and multimodal large language models in medicine?","Tool":"cypher_search"},{"index":66,"Answer":["michalowski m","topaz m","peltonen lm"],"Question":"Who are the authors of the research paper an ai-enabled nursing future with no documentation burden: a vision for a new reality?","Tool":"cypher_search"},{"index":67,"Answer":["mllms","llm","model","survey","llm mllms"],"Question":"What keywords are associated with the paper a comprehensive survey of large language models and multimodal large language models in medicine?","Tool":"cypher_search"},{"index":68,"Answer":"authored_by","Question":"What is the relationship between the author topaz m and the paper an ai-enabled nursing future with no documentation burden: a vision for a new reality?","Tool":"cypher_search"},{"index":69,"Answer":"published_in","Question":"How is the paper huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale linked to the year 2024?","Tool":"cypher_search"},{"index":70,"Answer":"has_keyword","Question":"What connection exists between the keyword llm and the paper a comprehensive survey of large language models and multimodal large language models in medicine?","Tool":"cypher_search"},{"index":71,"Answer":"indexed_at","Question":"In what way is the database arxiv related to the paper from text to multimodality: exploring the evolution and impact of large language models in medical practice?","Tool":"cypher_search"},{"index":72,"Answer":"False. Authors that published that paper are: ['niu q', 'chen k', 'li m', 'feng p', 'bi z', 'yan lk', 'zhang y', 'yin ch', 'fei c', 'liu j', 'peng b', 'wang t', 'wang y', 'chen s', 'liu m']","Question":"Was the paper from text to multimodality: exploring the evolution and impact of large language models in medical practice authored by michalowski m","Tool":"cypher_search"},{"index":73,"Answer":"False. The following keywords are used to describe that paper: ['documentation', 'patient', 'large', 'care', 'multimodal']","Question":"Is the keyword medical used to describe the paper an ai-enabled nursing future with no documentation burden: a vision for a new reality?","Tool":"cypher_search"},{"index":74,"Answer":"True. The paper was published in ['2024']","Question":"Was the paper huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale published in the year 2024?","Tool":"cypher_search"},{"index":75,"Answer":"False. The paper was indexed at ['arxiv']","Question":"Is the paper from text to multimodality: exploring the evolution and impact of large language models in medical practice indexed in the database pubmed?","Tool":"cypher_search"},{"index":76,"Answer":"True. The papers from the author zhang y that contain the keyword large language are: ['from text to multimodality: exploring the evolution and impact of large language models in medical practice']","Question":"Did the author zhang y write any paper that contains the keyword large language?","Tool":"cypher_search"},{"index":77,"Answer":"True. The keyword medical is associated with the following papers published in 2024: ['a comprehensive survey of large language models and multimodal large language models in medicine', 'from text to multimodality: exploring the evolution and impact of large language models in medical practice', 'huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale']","Question":"Was the keyword medical associated with any paper published in the year 2024?","Tool":"cypher_search"},{"index":78,"Answer":"False. The author wang t has following papers indexed at the pubmed database: []","Question":"Has the author wang t published any paper indexed in the database pubmed?","Tool":"cypher_search"},{"index":79,"Answer":"False. The keyword mllms was used in the following papers at the pubmed database: []","Question":"Has the keyword mllms been used in any paper published in the database pubmed?","Tool":"cypher_search"},{"index":80,"Answer":"Large Language Models (LLMs) have rapidly evolved from text-based systems to\nmultimodal platforms, significantly impacting various sectors including\nhealthcare. This comprehensive review explores the progression of LLMs to\nMultimodal Large Language Models (MLLMs) and their growing influence in medical\npractice. We examine the current landscape of MLLMs in healthcare, analyzing\ntheir applications across clinical decision support, medical imaging, patient\nengagement, and research. The review highlights the unique capabilities of\nMLLMs in integrating diverse data types, such as text, images, and audio, to\nprovide more comprehensive insights into patient health. We also address the\nchallenges facing MLLM implementation, including data limitations, technical\nhurdles, and ethical considerations. By identifying key research gaps, this\npaper aims to guide future investigations in areas such as dataset development,\nmodality alignment methods, and the establishment of ethical guidelines. As\nMLLMs continue to shape the future of healthcare, understanding their potential\nand limitations is crucial for their responsible and effective integration into\nmedical practice.","Question":"What's the abstract of the paper from text to multimodality: exploring the evolution and impact of large language models in medical practice?","Tool":"cypher_search"},{"index":81,"Answer":"2406.19280v4","Question":"What's the DOI of the paper huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale?","Tool":"cypher_search"},{"index":82,"Answer":16,"Question":"How many keywords were used?","Tool":"cypher_search"},{"index":83,"Answer":["arxiv","pubmed"],"Question":"Which are the databases available?","Tool":"cypher_search"},{"index":84,"Answer":"2024","Question":"Which year was the paper titled from text to multimodality: exploring the evolution and impact of large language models in medical practice published in?","Tool":"cypher_search"},{"index":85,"Answer":"arxiv","Question":"Which database indexed the paper a comprehensive survey of large language models and multimodal large language models in medicine?","Tool":"cypher_search"},{"index":86,"Answer":["xiao h","zhou f","liu x","liu t","li z","huang x"],"Question":"Who are the authors of the research paper a comprehensive survey of large language models and multimodal large language models in medicine?","Tool":"cypher_search"},{"index":87,"Answer":["mllms","llm","model","survey","llm mllms"],"Question":"What keywords are associated with the paper a comprehensive survey of large language models and multimodal large language models in medicine?","Tool":"cypher_search"},{"index":88,"Answer":"authored_by","Question":"What is the relationship between the author zhang y and the paper from text to multimodality: exploring the evolution and impact of large language models in medical practice?","Tool":"cypher_search"},{"index":89,"Answer":"published_in","Question":"How is the paper a comprehensive survey of large language models and multimodal large language models in medicine linked to the year 2024?","Tool":"cypher_search"},{"index":90,"Answer":"has_keyword","Question":"What connection exists between the keyword multimodal and the paper huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale?","Tool":"cypher_search"},{"index":91,"Answer":"indexed_at","Question":"In what way is the database arxiv related to the paper huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale?","Tool":"cypher_search"},{"index":92,"Answer":"False. Authors that published that paper are: ['michalowski m', 'topaz m', 'peltonen lm']","Question":"Was the paper an ai-enabled nursing future with no documentation burden: a vision for a new reality authored by gao a","Tool":"cypher_search"},{"index":93,"Answer":"False. The following keywords are used to describe that paper: ['medical', 'data', 'pubmedvision', 'mllms', 'multimodal']","Question":"Is the keyword documentation used to describe the paper huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale?","Tool":"cypher_search"},{"index":94,"Answer":"True. The paper was published in ['2024']","Question":"Was the paper a comprehensive survey of large language models and multimodal large language models in medicine published in the year 2025?","Tool":"cypher_search"},{"index":95,"Answer":"True. The paper was indexed at ['pubmed']","Question":"Is the paper an ai-enabled nursing future with no documentation burden: a vision for a new reality indexed in the database pubmed?","Tool":"cypher_search"},{"index":96,"Answer":"True. The papers from the author wang x that contain the keyword mllms are: ['huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale']","Question":"Did the author wang x write any paper that contains the keyword mllms?","Tool":"cypher_search"},{"index":97,"Answer":"True. The keyword mllms is associated with the following papers published in 2025: ['an ai-enabled nursing future with no documentation burden: a vision for a new reality']","Question":"Was the keyword mllms associated with any paper published in the year 2025?","Tool":"cypher_search"},{"index":98,"Answer":"True. The author wan x has following papers indexed at the arxiv database: ['huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale']","Question":"Has the author wan x published any paper indexed in the database arxiv?","Tool":"cypher_search"},{"index":99,"Answer":"True. The keyword multimodal was used in the following papers at the arxiv database: ['huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale']","Question":"Has the keyword multimodal been used in any paper published in the database arxiv?","Tool":"cypher_search"},{"index":100,"Answer":"Since the release of ChatGPT and GPT-4, large language models (LLMs) and\nmultimodal large language models (MLLMs) have attracted widespread attention\nfor their exceptional capabilities in understanding, reasoning, and generation,\nintroducing transformative paradigms for integrating artificial intelligence\ninto medicine. This survey provides a comprehensive overview of the\ndevelopment, principles, application scenarios, challenges, and future\ndirections of LLMs and MLLMs in medicine. Specifically, it begins by examining\nthe paradigm shift, tracing the transition from traditional models to LLMs and\nMLLMs, and highlighting the unique advantages of these LLMs and MLLMs in\nmedical applications. Next, the survey reviews existing medical LLMs and MLLMs,\nproviding detailed guidance on their construction and evaluation in a clear and\nsystematic manner. Subsequently, to underscore the substantial value of LLMs\nand MLLMs in healthcare, the survey explores five promising applications in the\nfield. Finally, the survey addresses the challenges confronting medical LLMs\nand MLLMs and proposes practical strategies and future directions for their\nintegration into medicine. In summary, this survey offers a comprehensive\nanalysis of the technical methodologies and practical clinical applications of\nmedical LLMs and MLLMs, with the goal of bridging the gap between these\nadvanced technologies and clinical practice, thereby fostering the evolution of\nthe next generation of intelligent healthcare systems.","Question":"What's the abstract of the paper a comprehensive survey of large language models and multimodal large language models in medicine?","Tool":"cypher_search"},{"index":101,"Answer":"2406.19280v4","Question":"What's the DOI of the paper huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale?","Tool":"cypher_search"},{"index":102,"Answer":16,"Question":"How many keywords were used?","Tool":"cypher_search"},{"index":103,"Answer":["arxiv","pubmed"],"Question":"Which are the databases available?","Tool":"cypher_search"},{"index":104,"Answer":"2025","Question":"Which year was the paper titled an ai-enabled nursing future with no documentation burden: a vision for a new reality published in?","Tool":"cypher_search"},{"index":105,"Answer":"arxiv","Question":"Which database indexed the paper from text to multimodality: exploring the evolution and impact of large language models in medical practice?","Tool":"cypher_search"},{"index":106,"Answer":["xiao h","zhou f","liu x","liu t","li z","huang x"],"Question":"Who are the authors of the research paper a comprehensive survey of large language models and multimodal large language models in medicine?","Tool":"cypher_search"},{"index":107,"Answer":["mllms","llm","model","survey","llm mllms"],"Question":"What keywords are associated with the paper a comprehensive survey of large language models and multimodal large language models in medicine?","Tool":"cypher_search"},{"index":108,"Answer":"authored_by","Question":"What is the relationship between the author wang y and the paper from text to multimodality: exploring the evolution and impact of large language models in medical practice?","Tool":"cypher_search"},{"index":109,"Answer":"published_in","Question":"How is the paper huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale linked to the year 2024?","Tool":"cypher_search"},{"index":110,"Answer":"has_keyword","Question":"What connection exists between the keyword documentation and the paper an ai-enabled nursing future with no documentation burden: a vision for a new reality?","Tool":"cypher_search"},{"index":111,"Answer":"indexed_at","Question":"In what way is the database arxiv related to the paper from text to multimodality: exploring the evolution and impact of large language models in medical practice?","Tool":"cypher_search"},{"index":112,"Answer":"False. Authors that published that paper are: ['xiao h', 'zhou f', 'liu x', 'liu t', 'li z', 'huang x']","Question":"Was the paper a comprehensive survey of large language models and multimodal large language models in medicine authored by topaz m","Tool":"cypher_search"},{"index":113,"Answer":"False. The following keywords are used to describe that paper: ['mllms', 'llm', 'model', 'survey', 'llm mllms']","Question":"Is the keyword large language used to describe the paper a comprehensive survey of large language models and multimodal large language models in medicine?","Tool":"cypher_search"},{"index":114,"Answer":"True. The paper was published in ['2024']","Question":"Was the paper from text to multimodality: exploring the evolution and impact of large language models in medical practice published in the year 2025?","Tool":"cypher_search"},{"index":115,"Answer":"True. The paper was indexed at ['arxiv']","Question":"Is the paper huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale indexed in the database arxiv?","Tool":"cypher_search"},{"index":116,"Answer":"True. The papers from the author chen j that contain the keyword mllms are: ['huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale']","Question":"Did the author chen j write any paper that contains the keyword mllms?","Tool":"cypher_search"},{"index":117,"Answer":"True. The keyword model is associated with the following papers published in 2024: ['a comprehensive survey of large language models and multimodal large language models in medicine', 'from text to multimodality: exploring the evolution and impact of large language models in medical practice', 'huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale']","Question":"Was the keyword model associated with any paper published in the year 2024?","Tool":"cypher_search"},{"index":118,"Answer":"True. The author wang x has following papers indexed at the arxiv database: ['huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale']","Question":"Has the author wang x published any paper indexed in the database arxiv?","Tool":"cypher_search"},{"index":119,"Answer":"False. The keyword medical was used in the following papers at the pubmed database: []","Question":"Has the keyword medical been used in any paper published in the database pubmed?","Tool":"cypher_search"}]}